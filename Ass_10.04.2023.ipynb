{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9388e1-e697-481b-be6c-ef919b8b2770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ad3cf-2f57-4752-918a-9154290f5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can use Bayes' theorem to calculate the probability that an employee is a smoker given that he/she uses the health insurance plan:\n",
    "\n",
    "P(smoker | uses insurance plan) = P(uses insurance plan | smoker) * P(smoker) / P(uses insurance plan)\n",
    "\n",
    "where P(uses insurance plan | smoker) is the probability that an employee uses the insurance plan given that he/she is a smoker, P(smoker) is the overall probability of an employee being a smoker, and P(uses insurance plan) is the overall probability of an employee using the insurance plan.\n",
    "\n",
    "From the problem statement, we know that P(uses insurance plan) = 0.7 and P(uses insurance plan | smoker) = 0.4. To find P(smoker), we need to use the law of total probability:\n",
    "\n",
    "P(smoker) = P(smoker and uses insurance plan) + P(smoker and does not use insurance plan)\n",
    "\n",
    "We know that 70% of employees use the insurance plan, so P(smoker and uses insurance plan) = 0.7 * 0.4 = 0.28. We don't know the proportion of non-smokers who use the insurance plan, but we can use the fact that the probabilities must add up to 1 to find it:\n",
    "\n",
    "P(non-smoker and uses insurance plan) = 1 - P(smoker and uses insurance plan) = 0.72\n",
    "\n",
    "Therefore, P(smoker and does not use insurance plan) = P(smoker) - P(smoker and uses insurance plan) = 0.32. We can now calculate P(smoker) as:\n",
    "\n",
    "P(smoker) = 0.28 + 0.32 = 0.6\n",
    "\n",
    "Now we can substitute these values into Bayes' theorem:\n",
    "\n",
    "P(smoker | uses insurance plan) = 0.4 * 0.6 / 0.7 = 0.343\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.343, or approximately 34.3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b0d53-7a45-4516-bfe2-2b72ed3683bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5762fa0-654d-4241-a189-e6f2f790b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the way they handle the features of the input data.\n",
    "\n",
    "Bernoulli Naive Bayes assumes that each feature is binary (i.e., takes on values of 0 or 1) and calculates the probability of each feature being 1 or 0 in each class. This makes it particularly suitable for problems where the input data is a binary vector (e.g., document classification problems where each word in a vocabulary is represented as either present or absent in a document). Bernoulli Naive Bayes is often used for text classification problems.\n",
    "\n",
    "On the other hand, Multinomial Naive Bayes assumes that each feature represents a count or frequency (e.g., the number of occurrences of a word in a document) and calculates the probability of each count or frequency in each class. This makes it particularly suitable for problems where the input data is a count vector (e.g., document classification problems where each word in a vocabulary is represented by its frequency of occurrence in a document).\n",
    "\n",
    "In summary, Bernoulli Naive Bayes is used for binary feature data, while Multinomial Naive Bayes is used for count or frequency feature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bee507-776e-476c-82b2-ef9a4ed47e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e5b369-7ec4-44ba-bfe0-b08697b7269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernoulli Naive Bayes assumes that each feature is binary (i.e., takes on values of 0 or 1) and calculates the probability of each feature being 1 or 0 in each class. In the case of missing values, we can either treat them as 0 or as a separate category altogether.\n",
    "\n",
    "If we treat missing values as 0, then we assume that the missing feature value is not present and assign it a value of 0. This approach may not be ideal in situations where the missing values may contain important information.\n",
    "\n",
    "Alternatively, we can treat missing values as a separate category and assign it a value of its own. This approach allows us to preserve the information that is present in the missing values and can be useful when we have reason to believe that the missing values are systematically different from the observed values.\n",
    "\n",
    "In practice, the approach to handling missing values in Bernoulli Naive Bayes may depend on the specific problem and the nature of the missing data. Other factors to consider include the amount of missing data, the relationship between the missing data and the outcome variable, and the degree of correlation between the missing data and the observed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ea8044-4a91-42cb-9ba0-b889e96055df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4579cfe0-c091-4f2a-a079-791f50a10694",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification problems. In the case of multi-class classification, Gaussian Naive Bayes calculates the probability of each class given the input features and selects the class with the highest probability as the predicted class.\n",
    "\n",
    "To perform multi-class classification using Gaussian Naive Bayes, we need to calculate the conditional probability of each class given the input features, P(class | features), for all possible classes. This involves calculating the class-specific mean and variance of each input feature, as well as the prior probability of each class.\n",
    "\n",
    "Once we have these probabilities, we can use Bayes' rule to calculate the posterior probability of each class given the input features, and select the class with the highest probability as the predicted class.\n",
    "\n",
    "In summary, Gaussian Naive Bayes can be used for multi-class classification by extending the algorithm to calculate the conditional probability of each class given the input features, and selecting the class with the highest probability as the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a11b0-890b-4f5a-9868-66fd7a9f0de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb05f8-e8fe-4bc5-841b-f2dc317e57aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "To perform the task, we would need to first download the \"Spambase Data Set\" from the UCI Machine Learning Repository and preprocess the data to prepare it for classification. This may involve tasks such as data cleaning, feature selection, and feature engineering. Once the data is ready, we can implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. We can then use 10-fold cross-validation to evaluate the performance of each classifier on the dataset.\n",
    "\n",
    "After running the classifiers, we would report the following performance metrics for each classifier: accuracy, precision, recall, and F1 score. These metrics would help us evaluate the performance of each classifier and compare them against each other.\n",
    "\n",
    "In the discussion section, we would analyze the results we obtained and discuss the performance of each classifier. We would also explore why one variant of Naive Bayes performed better than others and discuss the limitations of Naive Bayes that we observed during the analysis.\n",
    "\n",
    "In the conclusion section, we would summarize our findings and provide some suggestions for future work. For example, we may suggest exploring different feature engineering techniques or trying out other classification algorithms to improve the performance of the models. We may also suggest investigating the limitations of Naive Bayes in more detail and exploring ways to overcome them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3071996-c379-4edd-a9f8-bff9ab27caa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add8761-8ced-4947-8c59-4cf2eca765f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a909ae-c9f7-4a52-8303-96e35ee56091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e86083-3b50-46d3-b8bb-9f448512789b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de7c878-1557-47a3-8af4-cc8deb3b78fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245f6c7-9f62-449b-a967-74d7b25a39d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1d8bc9-1e39-4b81-862e-b6ffcad38ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45add6b-5ab7-417a-b2f3-2f8565e2c259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc64bb4-bbd5-4301-a3d1-a7a0c9e95d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79196b13-0bdb-4fbc-881d-adf985fc0f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6adc18-4b29-402c-b449-bb603130b6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f232b3-ba0d-44dc-864d-136cf3b4e87b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e44b5b-0467-44b8-b216-e8e5311a9d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5f176-15f8-4567-b5f9-f2d7bf255299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67549c37-1a24-454a-b606-04cff919ebec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a8fd0-6c78-4bab-8be9-69ab15123962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3037c3-99de-4aa1-9c84-6e977c881e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec76ac63-5ae3-4492-890e-e07dfc0ad422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
