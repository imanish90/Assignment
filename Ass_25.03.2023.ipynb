{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78f92c-e301-45b5-8240-94a2702d27f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf7e65-09f2-42e5-ab0d-d7879b17b055",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this dataset, we have 48 rows and 5 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c2fdd-f84e-491d-a7b2-784735fec6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31be6c41-180f-46be-94f2-475ccb857979",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Discern the distribution of a dataset\n",
    "Describe the shape of the data with basic statistics\n",
    "Make histograms\n",
    "Compare distributions with histograms\n",
    "Make box plots\n",
    "Using flight data, you'll learn how to better compare trends among airlines, adjusting your analysis based on how many flights an airline flies. By the end, you'll know which airlines and airports are more or less reliable—and maybe even make it to Thanksgiving on time this year!\n",
    "\n",
    "Loading data into Mode Python notebooks\n",
    "Mode is an analytics platform that brings together a SQL editor, Python notebook, and data visualization builder. Throughout this tutorial, you can use Mode for free to practice writing and running Python code.\n",
    "\n",
    "For this lesson, you'll be using records of United States domestic flights from the US Department of Transportation. To access the data, you’ll need to use a bit of SQL. Here’s how:\n",
    "\n",
    "Log into Mode or create an account.\n",
    "Navigate to this report and click Clone. This will take you to the SQL Query Editor, with a query and results pre-populated.\n",
    "Click Python Notebook under Notebook in the left navigation panel. This will open a new notebook, with the results of the query loaded in as a dataframe.\n",
    "The first input cell is automatically populated with datasets[0].head(n=5). Run this code so you can see the first five rows of the dataset.\n",
    "datasets[0] is a list object. Nested inside this list is a DataFrame containing the results generated by the SQL query you wrote. To learn more about how to access SQL queries in Mode Python Notebooks, read this documentation.\n",
    "\n",
    "Now you’re all ready to go.\n",
    "\n",
    "Getting oriented with the data\n",
    "This lesson uses data about US flight delays. For a data dictionary with more information, click here.\n",
    "\n",
    "After following the steps above, go to your notebook and import NumPy and Pandas, then assign your DataFrame to the data variable so it's easy to keep track of:\n",
    "\n",
    "Input\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "    \n",
    "Input\n",
    "\n",
    "data = datasets[0] # assign SQL query results to the data variable\n",
    "data = data.fillna(np.nan)\n",
    "    \n",
    "Input\n",
    "\n",
    "data.head() # our flight data, including delay minute counts by type, and total delay upon arrival\n",
    "    \n",
    "Output\n",
    "flight_date\tunique_carrier\tflight_num\torigin\tdest\tarr_delay\tcancelled\tdistance\tcarrier_delay\tweather_delay\tlate_aircraft_delay\tnas_delay\tsecurity_delay\tactual_elapsed_time\n",
    "0\t2015-01-02\tAA\t1\tJFK\tLAX\t-19.0\t0.0\t2475.0\tNaN\tNaN\tNaN\tNaN\tNaN\t381.0\n",
    "1\t2015-01-03\tAA\t1\tJFK\tLAX\t-39.0\t0.0\t2475.0\tNaN\tNaN\tNaN\tNaN\tNaN\t358.0\n",
    "2\t2015-01-04\tAA\t1\tJFK\tLAX\t-12.0\t0.0\t2475.0\tNaN\tNaN\tNaN\tNaN\tNaN\t385.0\n",
    "3\t2015-01-05\tAA\t1\tJFK\tLAX\t-8.0\t0.0\t2475.0\tNaN\tNaN\tNaN\tNaN\tNaN\t389.0\n",
    "4\t2015-01-06\tAA\t1\tJFK\tLAX\t25.0\t0.0\t2475.0\t0.0\t0.0\t0.0\t25.0\t0.0\t424.0\n",
    "Distributions\n",
    "Flying can be a frustrating and long endeavor in the U.S., especially around holidays and hurricane season when flights are delayed hours, sometimes days. As consumers, we're generally aware of those trends, but what about the day-to-day trends of air travel delays? Which airlines are doing better at getting you there on time, and which airports are the worst to fly out of?\n",
    "\n",
    "One of the best ways to answer questions like these is to look at the distributions of the relevant variables. You can think of the distribution of a dataset or variable as a list of possible values, and some indication as to how frequently each value occurs. For a quick refresher on distributions, check out this lesson.\n",
    "\n",
    "Before you look at the distributions of delays across airlines, start by exploring which airlines have the most delays.\n",
    "\n",
    "First, build the series indicating whether or not flights are delayed, just as you did in the previous lesson:\n",
    "\n",
    "Input\n",
    "\n",
    "data['delayed'] = data['arr_delay'].apply(lambda x: x > 0) #from previous lesson\n",
    "    \n",
    "Now count the number of delayed flights for each airline. Since you're only after one value for each airline, you don't have to use .groupby(), .size(), and .unstack() as in the previous lesson. Instead, just filter the dataset, the count up the rows for each carries using .value_counts():\n",
    "\n",
    "Input\n",
    "\n",
    "delayed_flights = data[data['delayed'] == True] #filter to only rows where delayer == True\n",
    "delayed_flights['unique_carrier'].value_counts() #count the number of rows for each carrier\n",
    "    \n",
    "Output\n",
    "\n",
    "WN    21150\n",
    "EV    11371\n",
    "OO    10804\n",
    "AA     9841\n",
    "DL     9803\n",
    "UA     8624\n",
    "MQ     8060\n",
    "US     6353\n",
    "B6     4401\n",
    "NK     2133\n",
    "AS     2104\n",
    "F9     1848\n",
    "HA     1354\n",
    "VX      781\n",
    "Name: unique_carrier, dtype: int64\n",
    "    \n",
    "It might be a good idea to look at the proportion of each airline's flights that were delayed, rather than just the total number of each airlines delayed flights.\n",
    "\n",
    "ABC of Analysis: Always Be Curious: A great analyst will always be skeptical and curious. When you reach a point in your analysis where you might have an answer, go a little further. Consider what might be affecting your results and what might support a counterargument. The best analysis will be presented as a very refined version of the broader investigation. Always be curious!\n",
    "\n",
    "Proportion of flights delayed\n",
    "To calculate the proportion of flights that were delayed, complete these 4 steps:\n",
    "\n",
    "1. Group by carrier and delayed\n",
    "Group flights by unique_carrier and delayed, get the count with .size() (as you did in the previous lesson, use .unstack() to return a DataFrame:\n",
    "\n",
    "Input\n",
    "\n",
    "data.groupby(['unique_carrier','delayed']).size().unstack()\n",
    "    \n",
    "Output\n",
    "delayed\tFalse\tTrue\n",
    "unique_carrier\t\t\n",
    "AA\t8912\t9841\n",
    "AS\t3527\t2104\n",
    "B6\t4832\t4401\n",
    "DL\t17719\t9803\n",
    "EV\t10596\t11371\n",
    "F9\t1103\t1848\n",
    "HA\t1351\t1354\n",
    "MQ\t4692\t8060\n",
    "NK\t1550\t2133\n",
    "OO\t9977\t10804\n",
    "UA\t7885\t8624\n",
    "US\t7850\t6353\n",
    "VX\t1254\t781\n",
    "WN\t21789\t21150\n",
    "Since you're going to use this for further analysis, it will be helpful to use .reset_index() to clean up the index. While you're at it, assign the new DataFrame to a variable.\n",
    "\n",
    "Input\n",
    "\n",
    "delayed_by_carrier = data.groupby(['unique_carrier','delayed']).size().unstack().reset_index()\n",
    "delayed_by_carrier[:5]\n",
    "    \n",
    "Output\n",
    "delayed\tunique_carrier\tFalse\tTrue\n",
    "0\tAA\t8912\t9841\n",
    "1\tAS\t3527\t2104\n",
    "2\tB6\t4832\t4401\n",
    "3\tDL\t17719\t9803\n",
    "4\tEV\t10596\t11371\n",
    "2. Create total flight count column\n",
    "Create a simple derived column that sums delayed and on-time flights for each carrier. If you want a refreshed on derived columns, return to this lesson:\n",
    "\n",
    "Input\n",
    "\n",
    "delayed_by_carrier['flights_count'] = (delayed_by_carrier[False] + delayed_by_carrier[True])\n",
    "delayed_by_carrier[:5]\n",
    "    \n",
    "Output\n",
    "delayed\tunique_carrier\tFalse\tTrue\tflights_count\n",
    "0\tAA\t8912\t9841\t18753\n",
    "1\tAS\t3527\t2104\t5631\n",
    "2\tB6\t4832\t4401\t9233\n",
    "3\tDL\t17719\t9803\t27522\n",
    "4\tEV\t10596\t11371\t21967\n",
    "3. Calculate the proportion of total flights delayed\n",
    "Divide the number of delayed flights by the total number of flights to get the proportion of delayed flights. Store this as another derived column:\n",
    "\n",
    "Input\n",
    "\n",
    "delayed_by_carrier['proportion_delayed'] = delayed_by_carrier[True] / delayed_by_carrier['flights_count']\n",
    "delayed_by_carrier[:4]\n",
    "    \n",
    "Output\n",
    "delayed\tunique_carrier\tFalse\tTrue\tflights_count\tproportion_delayed\n",
    "0\tAA\t8912\t9841\t18753\t0.524769\n",
    "1\tAS\t3527\t2104\t5631\t0.373646\n",
    "2\tB6\t4832\t4401\t9233\t0.476660\n",
    "3\tDL\t17719\t9803\t27522\t0.356188\n",
    "4. Sort by the proportion of flights delayed\n",
    "Finally, output the whole table, sorted by proportion of flights delayed by carrier:\n",
    "\n",
    "Input\n",
    "\n",
    "delayed_by_carrier.sort_values('proportion_delayed', ascending=False)\n",
    "    \n",
    "Output\n",
    "delayed\tunique_carrier\tFalse\tTrue\tflights_count\tproportion_delayed\n",
    "7\tMQ\t4692\t8060\t12752\t0.632058\n",
    "5\tF9\t1103\t1848\t2951\t0.626228\n",
    "8\tNK\t1550\t2133\t3683\t0.579147\n",
    "0\tAA\t8912\t9841\t18753\t0.524769\n",
    "10\tUA\t7885\t8624\t16509\t0.522382\n",
    "9\tOO\t9977\t10804\t20781\t0.519898\n",
    "4\tEV\t10596\t11371\t21967\t0.517640\n",
    "6\tHA\t1351\t1354\t2705\t0.500555\n",
    "13\tWN\t21789\t21150\t42939\t0.492559\n",
    "2\tB6\t4832\t4401\t9233\t0.476660\n",
    "11\tUS\t7850\t6353\t14203\t0.447300\n",
    "12\tVX\t1254\t781\t2035\t0.383784\n",
    "1\tAS\t3527\t2104\t5631\t0.373646\n",
    "3\tDL\t17719\t9803\t27522\t0.356188\n",
    "This paints a very different picture - Southwest is squarely in the middle of the pack when it comes to the percentage of flights delayed.\n",
    "\n",
    "Basic statistics: mean, median, percentiles\n",
    "But what about the length of time passengers are delayed on each airline? It's plausible that the proportion of flight delays may be lower for some airlines, but the length of those delays may be much longer. Passengers care more about longer flight delays than shorter ones, and they might choose a different airline given a record of long delays. For passengers, which airlines seem more reliable? Or in more measured terms, How many minutes are flights delayed on average, for each airline?\n",
    "\n",
    "Mean\n",
    "The mean, or the average, gives you a general idea of how many minutes flights were delayed for each airline. .pivot_table() calculates the mean of the aggregated values by default. You can pivot on the column unique_carrier to see the mean delay time aggregated by airline:\n",
    "\n",
    "Input\n",
    "\n",
    "data.pivot_table(columns='unique_carrier', values='arr_delay').sort_values(ascending=False)\n",
    "    \n",
    "Output\n",
    "\n",
    "unique_carrier\n",
    "MQ    35.627406\n",
    "F9    28.836953\n",
    "NK    22.779670\n",
    "OO    19.031663\n",
    "EV    18.358520\n",
    "UA    16.094772\n",
    "AA    15.616299\n",
    "B6    13.576129\n",
    "WN    11.273536\n",
    "US     7.671557\n",
    "HA     6.458937\n",
    "DL     4.118949\n",
    "VX     3.833908\n",
    "AS     1.731951\n",
    "Name: arr_delay, dtype: float64\n",
    "    \n",
    "Note that because .pivot_table calculated mean by default, the above is effectively the same as if you were to explicitly pass the argument like this (and it produces the exact same results):\n",
    "\n",
    "Input\n",
    "\n",
    "data.pivot_table(columns='unique_carrier', values='arr_delay', aggfunc=np.mean).sort_values(ascending=False)\n",
    "    \n",
    "Output\n",
    "\n",
    "unique_carrier\n",
    "MQ    35.627406\n",
    "F9    28.836953\n",
    "NK    22.779670\n",
    "OO    19.031663\n",
    "EV    18.358520\n",
    "UA    16.094772\n",
    "AA    15.616299\n",
    "B6    13.576129\n",
    "WN    11.273536\n",
    "US     7.671557\n",
    "HA     6.458937\n",
    "DL     4.118949\n",
    "VX     3.833908\n",
    "AS     1.731951\n",
    "Name: arr_delay, dtype: float64\n",
    "    \n",
    "Basic statistics with .describe()\n",
    ".describe() is a handy function when you’re working with numeric columns. You can use .describe() to see a number of basic statistics about the column, such as the mean, min, max, and standard deviation. This can give you a quick overview of the shape of the data.\n",
    "\n",
    "Before using describe(), select the arr_delay series for all Southwest flights:\n",
    "\n",
    "Input\n",
    "\n",
    "southwest = data[data['unique_carrier'] == 'WN']['arr_delay']\n",
    "    \n",
    "Output\n",
    "\n",
    "62030    18.0\n",
    "62031    11.0\n",
    "62032     9.0\n",
    "62033    44.0\n",
    "62034    42.0\n",
    "Name: arr_delay, dtype: float64\n",
    "    \n",
    "In case that was confusing, here's what just happened, evaluated inside-out, then left to right:\n",
    "\n",
    "data['unique_carrier'] == 'WN'] creates a boolean index that returns True for rows that represent Southwest flights\n",
    "Wrapping that in data[...] applies the boolean index to the DataFrame data.\n",
    "['arr_delay'] reduces the columns to just the ['arr_delay'] column (and the index).\n",
    "You can now run .describe() on this new object you've created to get basic statistics:\n",
    "\n",
    "Input\n",
    "\n",
    "southwest.describe()\n",
    "    \n",
    "Output\n",
    "\n",
    "count    42020.000000\n",
    "mean        11.273536\n",
    "std         36.438970\n",
    "min        -55.000000\n",
    "25%         -9.000000\n",
    "50%          1.000000\n",
    "75%         19.000000\n",
    "max        535.000000\n",
    "Name: arr_delay, dtype: float64\n",
    "    \n",
    "This is a feature that really sets python apart from SQL or Excel. It would take a lot of work to get this information in either of those tools, but here it is as easy as adding the .describe() method.\n",
    "\n",
    "Here's a quick breakdown of the above as it relates to this particular dataset:\n",
    "\n",
    "count: there are 42,020 rows in the dataset, which is filtered to only show Southwest (WN).\n",
    "mean: the average delay.\n",
    "std: the standard deviation. More on this below.\n",
    "min: the shortest delay in the dataset. In this case, the flight was very early.\n",
    "25%: the 25th percentile. 25% of delays were lower than -9.00.\n",
    "50%: the 50th percentile, or the median. 50% of delays were lower than 1.00.\n",
    "75%: the 75th percentile. 75% of delays were lower than 19.00.\n",
    "max: the longest delay in the dataset: 535.00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c4b564-0f83-42e7-95ca-ca5a1f20f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b32854-d100-4559-b13b-f30dbf18f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min, Max, and Range\n",
    "Study on\n",
    "Childhood Weight\n",
    "Statistic\tValue\n",
    "Count\t12\n",
    "Sum\t642\n",
    "Mean\t53.5\n",
    "Median\t45.5\n",
    "Mode\tNo Mode\n",
    "Min\t13\n",
    "Max\t110\n",
    "Range\t97\n",
    "Standard Deviation\t31.0\n",
    "The min and max are also useful for understand a given set of data...\n",
    "\n",
    "13        22        26        38        36        42\n",
    "49        50        77        81        98        110\n",
    "\n",
    "Using the dataset of child weights above, we can find the min and max. The min is simply the lowest observation, while the max is the highest observation. Obviously, it is easiest to determine the min and max if the data are ordered from lowest to highest. So for our data, the min is 13 and the max is 110.\n",
    "\n",
    "Using the Min and Max\n",
    "Finding the min and max helps us understand the total span of our data. There are a variety of reasons you might do this depending on the study. It’s usually something that is nice to know and helps you feel more familiar with your data. Or you can do a min and max to help you identify something wrong.\n",
    "\n",
    "For instance, maybe for our study of childhood weight, we really only want to focus on children over the age of two years old. By identifying the min of 13 pounds, we may be concerned that we have unknowingly gotten an infant in our study. We would then want to further investigate how this occurred, if there are other infants in our dataset, and what should be done with these data as we proceed with the data analyses.\n",
    "\n",
    "Range\n",
    "Sometimes it is also useful to use the min and max to calculate the range of a dataset. The range is a numerical indication of the span of our data. To calculate a range, simply subtract the min (13) from the max (110). The range for this dataset is 97. \n",
    "\n",
    "Range equals the Max minus the Min\n",
    "\n",
    "The range is another example of something that is nice to know, but isn’t all that useful unless we are comparing it with something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30572f3-b935-4d7b-b371-cf1c05b6218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063c54d7-e77b-47db-a25c-8c4e7cd545e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Abstract\n",
    "Have you ever wondered why plane ticket prices seem to jump around at random? This is actually the product of a carefully calculated algorithm that airlines use to maximize their profits by balancing individual ticket pricing and the capacity of the plane. The price of a plane ticket is constantly changing based on current demand for a flight, the number of seats available, and the timing of booking. Although the algorithm itself is quite complex, there are a few easy steps that can be taken in order to ensure you get the best possible deal for a flight. \n",
    "\n",
    "Bumping\n",
    "Most of us are likely familiar with the idea of “bumping,” when an airline overbooks a flight and, in best case, has to “bribe” a couple of passengers with vouchers and cash to take a later flight, or, in worst case, has to physically pull them off the plane. United has been under fire lately for a couple of incidents in which passengers were forcibly removed from overbooked flights. Despite the recent news flood of bumping stories, bumping is actually quite rare, occurring to only about 0.06% of flyers every year [10]. While it may seem that bumping is the result of a mistake by the airline’s attendants or computer system, it is actually a very well calculated gamble on how to achieve maximum revenue. Airlines try to fill each aircraft to capacity to ensure there are no empty seats, or lost potential revenue, and for every flight they bet that a handful of passengers either cancel or don’t show up. Most of the time, the airlines bet correctly and the aircraft ends up nearly full and no one gets bumped. This allows airlines to fill their flights as much as possible, making sure that the load of the flight (the number of passengers that actually take the flight) is as close as possible to the capacity (the number of seats on the aircraft). The lost revenue associated with every seat left empty by a no-show is more than just the cost of the seat because it also includes all of the potential passengers willing to buy that seat who were turned away and whose business the airline has therefore lost to a competitor. These passengers who are turned away are called the “spill:” the difference between demand, the number of potential passengers interested in booking the flight, and load [5], [9]. There is often spill even when a flight is full because there are more people interested in the flight than there are seats. While airlines try their best to minimize spill by supplying enough seats to meet the demand for a flight, right now they are left to guesswork. This is because it is nearly impossible to know exactly how many people want to book a seat for each flight. Today, with new methods of internet user tracking permeating throughout many industries, this guesswork may soon become a simple formula. \n",
    "\n",
    "Protecting Seats\n",
    "Airlines work to minimize the revenue loss from spill by trying to push low-paying passengers into the spill group instead of high-paying passengers. They do this by protecting, or reserving, a group of economy seats on every flight for “full fare” flyers. Instead of letting an entire flight fill up far in advance with lower priced tickets, airlines reserve some seats so that last minute bookers, paying the higher last-minute price, can buy these seats and increase the revenue of the airline for that flight. The tricky part comes when an airline has to decide how many seats to protect and for how long. If the airline doesn’t reserve enough seats, some high-paying flyers might be turned away as the flight is already overbooked. On the other side, if there aren’t enough last-minute bookers then the flight could be under-booked despite having a large spill of low-paying customers. Even when running predictive models to optimize revenue losses associated with under-booking (empty seats), spill (refused customers), and overbooking (bribing vouchers and customer dissatisfaction), many airlines still struggle to exceed a 5% operating margin [4].\n",
    "\n",
    "Like many airlines, American Airlines employs the protecting method for high paying, last-minute customers. These seats are not visible to the average customer without elite status because he or she is likely to book in advance at a lower price or use miles or points to cover a portion of the fare. However, these seats are visible to customers with elite status as they are more likely to reserve seats last minute at a higher price, especially since many with elite status are business flyers who are willing to pay a higher price for certain times compared to more flexible leisure travelers. But it is worth noting that these seats become open to all flyers at check-in 24 hours before the flight [7].\n",
    "\n",
    "Buckets\n",
    "When determining how many seats to protect for full fare economy passengers, airlines must account for not only the static capacity and load, but also a dynamic, or constantly changing, demand. This demand varies between and within each flight depending on its date, time of day, origin, and destination. For example, popular flights for commuters often have high demand a few days before the flight and thus more seats are protected, but leisure flights are more likely to have lower demand as the date of the flight grows closer. This case of higher demand is mirrored in more popular vacation destinations and in weekend flights as compared to mid-week flights [5]. Figure 1 shows how dynamic pricing (right) is able to both increase airline profits and decrease consumer fares by increasing the load factor. Figure 2 depicts the inverse relationship between load factor and fare, and that by adjusting the fares according to the real-time load factor airlines can increase their revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f208f4e5-8c26-40dc-b65c-fc21b35fb7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28be8d65-8575-4031-b6e5-63f03b0c061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "As an AI language model, I don't have access to the dataset that you're referring to. However, I can give you general information on how to identify outliers using a boxplot and how they may impact your analysis.\n",
    "\n",
    "A boxplot is a graphical representation of data that shows the distribution of the dataset by displaying the minimum, first quartile, median, third quartile, and maximum values. It also includes any potential outliers, which are data points that are significantly different from the rest of the dataset. To identify outliers using a boxplot, you need to look for any points that are outside the whiskers of the boxplot.\n",
    "\n",
    "If there are outliers in the dataset, they may impact your analysis in a few ways. Outliers can skew the mean, which is a measure of central tendency, making it an unreliable indicator of the average value in the dataset. Outliers can also affect the standard deviation, which is a measure of the spread of the data, making it an inaccurate representation of the variability of the dataset. Additionally, outliers can impact statistical tests, such as hypothesis tests or regression analysis, by influencing the results and potentially leading to false conclusions. Therefore, it's important to carefully consider outliers when analyzing data and decide whether to remove them or account for them in your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729028a5-ebff-4cdf-b754-4a4494f4e44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1fb69-24c5-4b81-beaf-90be99a3642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "To identify the peak travel season from the Flight Price dataset, I would analyze the following features:\n",
    "\n",
    "Time of Year: I would look at the time of year when the flights were booked and identify which months had the highest number of bookings.\n",
    "\n",
    "Destination: I would analyze the data by destination and identify which cities or regions had the highest number of bookings during specific periods.\n",
    "\n",
    "Price: I would also analyze the data by price and identify any trends in pricing during specific periods.\n",
    "\n",
    "To present my findings to my boss, I would prepare a report summarizing my analysis and include the following information:\n",
    "\n",
    "A graph or chart showing the distribution of flights booked by month, highlighting the months with the highest number of bookings.\n",
    "\n",
    "A breakdown of the top destinations and the number of flights booked for each during specific periods, with a focus on peak periods.\n",
    "\n",
    "A graph or chart showing the average flight price by month or period, highlighting any trends or patterns.\n",
    "\n",
    "A summary of my analysis and recommendation for the peak travel season based on the data.\n",
    "\n",
    "By presenting this information to my boss, they would have a clear understanding of when the peak travel season is, and they could use this information to make more informed decisions about travel planning and pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f828a78-c8c3-4a0c-bf67-ffcd587425b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a4223-ff1c-41f0-b971-b85d46ec3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "To identify trends in flight prices from the Flight Price dataset, I would analyze the following features:\n",
    "\n",
    "Time of Year: I would look at the time of year when the flights were booked and identify any trends in pricing during specific months or seasons.\n",
    "\n",
    "Day of the Week: I would analyze the data by the day of the week and identify any trends in pricing for specific days of the week.\n",
    "\n",
    "Airline Carrier: I would also analyze the data by airline carrier and identify any trends in pricing across different carriers.\n",
    "\n",
    "Flight Duration: I would analyze the data by flight duration and identify any trends in pricing for shorter or longer flights.\n",
    "\n",
    "To present my findings to my team, I would use the following visualizations:\n",
    "\n",
    "A line chart showing the average flight price by month or season, highlighting any trends or patterns.\n",
    "\n",
    "A bar chart showing the average flight price by day of the week, highlighting any differences in pricing between weekdays and weekends.\n",
    "\n",
    "A box plot showing the distribution of flight prices by airline carrier, highlighting any differences in pricing between carriers.\n",
    "\n",
    "A scatter plot showing the relationship between flight duration and price, highlighting any trends in pricing for shorter or longer flights.\n",
    "\n",
    "By presenting these visualizations to my team, they would have a clear understanding of the trends in flight prices and could use this information to make more informed decisions about pricing and marketing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b19a55-96e5-4153-8891-10680e78fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a9e85-22bd-4279-995c-77c7169a30e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "To identify the factors that affect flight prices from the Flight Price dataset, I would analyze the following features:\n",
    "\n",
    "Time of Year: I would analyze the data by the time of year when the flights were booked and identify any patterns in pricing during specific months or seasons.\n",
    "\n",
    "Departure and Destination: I would analyze the data by the departure and destination cities and identify any patterns in pricing for specific routes.\n",
    "\n",
    "Airline Carrier: I would also analyze the data by airline carrier and identify any patterns in pricing across different carriers.\n",
    "\n",
    "Flight Duration: I would analyze the data by flight duration and identify any patterns in pricing for shorter or longer flights.\n",
    "\n",
    "Advanced Booking: I would analyze the data by the time between booking and departure and identify any patterns in pricing for advanced bookings.\n",
    "\n",
    "Number of Stops: I would analyze the data by the number of stops and identify any patterns in pricing for direct or connecting flights.\n",
    "\n",
    "To present my findings to the management team, I would create a report summarizing my analysis and include the following information:\n",
    "\n",
    "A summary of the factors that affect flight prices, highlighting the top factors that have the most significant impact on pricing.\n",
    "\n",
    "A table or chart showing the average flight price by month or season, departure and destination cities, airline carrier, flight duration, advanced booking, and number of stops, highlighting any trends or patterns.\n",
    "\n",
    "A regression analysis to determine the relationship between flight prices and the factors identified in the analysis.\n",
    "\n",
    "A summary of the results of the regression analysis, highlighting the factors that have the most significant impact on flight prices.\n",
    "\n",
    "By presenting this information to the management team, they would have a clear understanding of the factors that affect flight prices, and they could use this information to make more informed decisions about pricing and marketing strategies. Additionally, they could use the results of the regression analysis to develop pricing models that take into account the factors that affect flight prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a5bfa5-9461-4c36-8749-58877290dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e4634-b23d-465b-a3e7-3eadad9d3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "As an AI language model, I do not have access to the local file system to load the dataset. However, I can provide you with the code to load the dataset and determine its dimensions once you have it available. Here's a Python code snippet that can load the Google Playstore dataset using Pandas and display its dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fdd60f-d2dc-45e6-b172-b0671a9ab644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Google Playstore dataset\n",
    "df = pd.read_csv('googleplaystore.csv')\n",
    "\n",
    "# Display the dimensions of the dataset\n",
    "print(\"Number of rows: \", df.shape[0])\n",
    "print(\"Number of columns: \", df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6dca40-5dae-4145-a315-98c923dbb8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code will display the number of rows and columns of the loaded dataset. You just need to ensure that the path of the dataset file is correct when passing it to the read_csv() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45afddd6-9d81-46f5-bb73-fc8083a1a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b514b1-27fe-45e8-bfe2-0f1d023b1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "To create a boxplot to compare the ratings of different app categories, we can use the Google Playstore dataset, which includes information about different apps, including their categories and ratings.\n",
    "\n",
    "Here's a Python code snippet that can create a boxplot to compare the ratings of different app categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630a0c8-5222-46f4-873d-99c4f7c37764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Google Playstore dataset\n",
    "df = pd.read_csv('googleplaystore.csv')\n",
    "\n",
    "# Group the data by category and get the mean rating for each category\n",
    "ratings_by_category = df.groupby('Category')['Rating'].mean().reset_index()\n",
    "\n",
    "# Create a boxplot to compare the ratings of different app categories\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.boxplot(df.groupby('Category')['Rating'].apply(list))\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Rating')\n",
    "plt.title('Ratings by Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae072f3-53c6-42d3-a8e7-05823c0406e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code will create a boxplot that shows the distribution of ratings for each app category. The x-axis represents the different app categories, and the y-axis represents the rating. The boxplot shows the median, interquartile range, and any outliers for each app category.\n",
    "\n",
    "By examining this boxplot, we can see how the ratings of apps vary by category. We can also identify any outliers and compare the distribution of ratings between different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817e15e4-5ca9-4dba-a549-8aaad2cff329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cedc233-a20b-4d76-bbb5-0b8b3a54999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "To identify missing values in the Google Playstore dataset, we can use the isnull() function in Pandas, which returns a DataFrame with the same shape as the original dataset but with boolean values indicating if a particular cell is null or missing. We can then use the sum() function to count the number of missing values for each column. Here's the Python code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c837bdc8-6a31-49b8-9cdf-956d4bc0f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Google Playstore dataset\n",
    "df = pd.read_csv('googleplaystore.csv')\n",
    "\n",
    "# Identify missing values in the dataset\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Print the number of missing values for each column\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42ea6a-8d1f-434a-b34b-7da503acdd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "By running this code, we can see the number of missing values for each column in the dataset. Depending on the number of missing values, they can impact our analysis in different ways:\n",
    "\n",
    "If there are only a few missing values, we can consider dropping the corresponding rows or filling them with some reasonable value (e.g., mean, median, or mode).\n",
    "\n",
    "If a significant portion of the data is missing, we might consider removing the entire column or imputing values using more advanced techniques (e.g., KNN imputation or regression imputation).\n",
    "\n",
    "If the missing values are not randomly distributed but rather systematic (e.g., only for certain categories or time periods), this could introduce bias and make our analysis less accurate.\n",
    "\n",
    "Therefore, it's essential to identify missing values in the dataset and handle them appropriately to avoid any potential bias or inaccuracies in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211ad61-2b4f-4a74-8201-9cddadae4dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf70a0-274b-4e88-9cc3-2bfc78857e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "To examine the relationship between the size of an app and its rating, we can use the Google Playstore dataset, which includes information about different apps, including their size and ratings.\n",
    "\n",
    "Here's a Python code snippet that can create a scatter plot to visualize the relationship between app size and rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff9dcb-7e33-4d62-8ae8-aaf37093e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Google Playstore dataset\n",
    "df = pd.read_csv('googleplaystore.csv')\n",
    "\n",
    "# Select the 'Size' and 'Rating' columns and drop any rows with missing values\n",
    "df = df[['Size', 'Rating']].dropna()\n",
    "\n",
    "# Convert the 'Size' column to numeric values in bytes\n",
    "df['Size'] = df['Size'].apply(lambda x: int(x[:-1]) * 1024 if x[-1] == 'M' else int(x))\n",
    "\n",
    "# Create a scatter plot to visualize the relationship between app size and rating\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(df['Size'], df['Rating'])\n",
    "plt.xlabel('App Size (bytes)')\n",
    "plt.ylabel('Rating')\n",
    "plt.title('Relationship between App Size and Rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a697da13-ffce-4a62-94b1-4c9a61ae9846",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code will create a scatter plot that shows the relationship between the size of an app and its rating. The x-axis represents the size of the app in bytes, and the y-axis represents the rating. Each dot on the scatter plot represents an app.\n",
    "\n",
    "By examining this scatter plot, we can see if there is any relationship between the size of an app and its rating. We can also identify any outliers or trends in the data that may impact our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a86fa-187a-4388-b28c-063aae86f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa0e15-c2d4-45c2-b4d3-7dceb833e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "To compare the average prices of apps by type, we can use the Google Playstore dataset, which includes information about different apps, including their prices and types.\n",
    "\n",
    "Here's a Python code snippet that can create a bar chart to compare average prices by app type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06da1d9-befa-475c-96a0-9554a1bee1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Google Playstore dataset\n",
    "df = pd.read_csv('googleplaystore.csv')\n",
    "\n",
    "# Select the 'Type' and 'Price' columns and drop any rows with missing values\n",
    "df = df[['Type', 'Price']].dropna()\n",
    "\n",
    "# Convert the 'Price' column to numeric values\n",
    "df['Price'] = df['Price'].apply(lambda x: float(x[1:]))\n",
    "\n",
    "# Group the data by app type and calculate the average price for each group\n",
    "grouped_data = df.groupby('Type')['Price'].mean()\n",
    "\n",
    "# Create a bar chart to compare average prices by app type\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(grouped_data.index, grouped_data.values)\n",
    "plt.xlabel('App Type')\n",
    "plt.ylabel('Average Price ($)')\n",
    "plt.title('Average Prices by App Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac9edd8-e6e5-4ee3-8466-0d291a18db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code will create a bar chart that shows the average prices of apps by type. The x-axis represents the different app types, and the y-axis represents the average price in dollars. Each bar on the bar chart represents the average price of a particular app type.\n",
    "\n",
    "By examining this bar chart, we can see if there is any relationship between the type of app and its price. We can also identify any differences in the average price of different app types that may impact our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6de7ae4-4cd9-4d87-9fa4-18be3fe39eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6036e752-a472-4a7f-b39a-f6d097e21221",
   "metadata": {},
   "outputs": [],
   "source": [
    "To identify the top 10 most popular apps in the Google Playstore dataset, we can create a frequency table of the number of installs for each app and sort it in descending order.\n",
    "\n",
    "Here's a Python code snippet that can create a frequency table of the number of installs for each app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683f30b-ad97-4145-b394-18a420bf499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Google Playstore dataset\n",
    "df = pd.read_csv('googleplaystore.csv')\n",
    "\n",
    "# Drop any rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Remove any special characters from the 'Installs' column and convert it to numeric values\n",
    "df['Installs'] = df['Installs'].apply(lambda x: int(x.replace('+', '').replace(',', '')))\n",
    "\n",
    "# Create a frequency table of the number of installs for each app and sort it in descending order\n",
    "freq_table = df[['App', 'Installs']].groupby('App').sum().sort_values('Installs', ascending=False)\n",
    "\n",
    "# Select the top 10 most popular apps\n",
    "top_10 = freq_table.head(10)\n",
    "\n",
    "# Print the top 10 most popular apps\n",
    "print(top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec94b69-762e-4669-bf0e-f9d98d90a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code will create a frequency table of the number of installs for each app, sort it in descending order, and select the top 10 most popular apps. The resulting table will show the name of each app and the total number of installs for that app.\n",
    "\n",
    "By examining this frequency table, we can identify the top 10 most popular apps in the dataset based on the number of installs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62737960-cd3f-444f-8999-813878801018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d16388-624c-43fc-b27a-06d1d5a0eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "To identify the most popular app categories in the Google Playstore dataset, we can approach the task by analyzing the frequency of app categories and the number of installs for each category. Here are the steps we can take:\n",
    "\n",
    "Load the Google Playstore dataset and select the relevant columns: 'Category' and 'Installs'.\n",
    "Clean the data by removing any missing values and special characters from the 'Installs' column.\n",
    "Convert the 'Installs' column to numeric values.\n",
    "Group the data by app category and calculate the total number of installs for each category.\n",
    "Sort the data in descending order based on the total number of installs for each category.\n",
    "Present the results in a graph or table format to the company.\n",
    "Here's a Python code snippet that can perform these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa217aa-018a-406d-8aed-44b67ad8e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Google Playstore dataset and select the 'Category' and 'Installs' columns\n",
    "df = pd.read_csv('googleplaystore.csv', usecols=['Category', 'Installs'])\n",
    "\n",
    "# Drop any rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Remove any special characters from the 'Installs' column and convert it to numeric values\n",
    "df['Installs'] = df['Installs'].apply(lambda x: int(x.replace('+', '').replace(',', '')))\n",
    "\n",
    "# Group the data by app category and calculate the total number of installs for each category\n",
    "grouped_data = df.groupby('Category')['Installs'].sum()\n",
    "\n",
    "# Sort the data in descending order based on the total number of installs for each category\n",
    "sorted_data = grouped_data.sort_values(ascending=False)\n",
    "\n",
    "# Create a bar chart to visualize the results\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(sorted_data.index, sorted_data.values)\n",
    "plt.xlabel('App Category')\n",
    "plt.ylabel('Total Installs')\n",
    "plt.title('Total Installs by App Category')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Print the sorted data table\n",
    "print(sorted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561df353-6beb-4770-9be3-d7d1671d23c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code will create a bar chart to visualize the total number of installs for each app category and print the sorted data table. By examining the results, we can identify the most popular app categories and make recommendations to the company based on our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b59befb-dc6b-4eb5-9a47-f4cff26b3cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e9c0a-f3ca-4fd0-a018-fb677a2ec78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "To identify the most successful app developers in the Google Playstore dataset, we can analyze several features. Here are some of the features that we can analyze:\n",
    "\n",
    "Number of apps developed by each developer\n",
    "Total number of installs for each developer's apps\n",
    "Average rating of each developer's apps\n",
    "Price range of each developer's apps\n",
    "Category distribution of each developer's apps\n",
    "We can use data visualizations to present our findings in a more meaningful way. Here are some data visualizations that can be useful:\n",
    "\n",
    "A bar chart to show the number of apps developed by each developer.\n",
    "A scatter plot to show the relationship between the total number of installs and the average rating of each developer's apps.\n",
    "A box plot to show the distribution of app prices for each developer.\n",
    "A stacked bar chart to show the category distribution of each developer's apps.\n",
    "Here's a Python code snippet that can perform these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01218194-38a3-46b9-8454-ea4cbeabd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Google Playstore dataset and select the relevant columns\n",
    "df = pd.read_csv('googleplaystore.csv', usecols=['App', 'Category', 'Rating', 'Installs', 'Price', 'Developer'])\n",
    "\n",
    "# Drop any rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Group the data by developer and calculate the relevant metrics\n",
    "grouped_data = df.groupby('Developer').agg({\n",
    "    'App': 'count',\n",
    "    'Installs': 'sum',\n",
    "    'Rating': 'mean',\n",
    "    'Price': 'mean',\n",
    "    'Category': lambda x: x.value_counts().index[0]\n",
    "}).rename(columns={'App': 'Number of Apps', 'Installs': 'Total Installs', 'Rating': 'Average Rating', 'Price': 'Average Price', 'Category': 'Most Common Category'})\n",
    "\n",
    "# Sort the data in descending order based on the total number of installs for each developer\n",
    "sorted_data = grouped_data.sort_values(by='Total Installs', ascending=False)\n",
    "\n",
    "# Create data visualizations to present the findings\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(sorted_data.index, sorted_data['Number of Apps'])\n",
    "plt.xlabel('Developer')\n",
    "plt.ylabel('Number of Apps')\n",
    "plt.title('Number of Apps by Developer')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(sorted_data['Total Installs'], sorted_data['Average Rating'])\n",
    "plt.xlabel('Total Installs')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Relationship between Total Installs and Average Rating')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.boxplot(sorted_data['Average Price'])\n",
    "plt.ylabel('Average Price')\n",
    "plt.title('Distribution of Average Price by Developer')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "stacked_data = pd.DataFrame(df.groupby(['Developer', 'Category'])['App'].count()).unstack().fillna(0)\n",
    "stacked_data.plot(kind='bar', stacked=True, figsize=(10,6))\n",
    "plt.xlabel('Developer')\n",
    "plt.ylabel('Number of Apps')\n",
    "plt.title('Category Distribution of Apps by Developer')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Print the sorted data table\n",
    "print(sorted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf723af4-e675-4945-95e0-f9653073f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code will create several data visualizations, including a bar chart to show the number of apps developed by each developer, a scatter plot to show the relationship between the total number of installs and the average rating of each developer's apps, a box plot to show the distribution of app prices for each developer, and a stacked bar chart to show the category distribution of each developer's apps. By examining the results, we can identify the most successful app developers and make recommendations to the company based on our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca710591-2054-48cf-bdf2-1bd2118a1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a858c63-2bf5-45fb-a5f5-a4bfa2309933",
   "metadata": {},
   "outputs": [],
   "source": [
    "To identify the best time to launch a new app, we can analyze the dataset based on the following features:\n",
    "\n",
    "App downloads: We can analyze the number of downloads of different apps to identify the months when the app downloads are the highest.\n",
    "\n",
    "App reviews and ratings: We can analyze the number of reviews and ratings for different apps to identify the months when the reviews and ratings are the highest.\n",
    "\n",
    "App updates: We can analyze the frequency of app updates to identify the months when most developers are releasing updates to their apps.\n",
    "\n",
    "To present our findings to the marketing research firm, we can use the following data visualizations:\n",
    "\n",
    "Line charts: We can use line charts to show the trend of app downloads, reviews, and ratings over time.\n",
    "\n",
    "Bar charts: We can use bar charts to compare the number of app downloads, reviews, and ratings across different months.\n",
    "\n",
    "Scatter plots: We can use scatter plots to show the relationship between the number of app downloads and reviews.\n",
    "\n",
    "Based on the analysis of these features and visualizations, we can make recommendations to the marketing research firm on the best time to launch a new app. For example, we may find that the months with the highest number of app downloads and reviews are the months with the most potential for launching a new app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597193ea-9e7f-4f70-a125-48bce7c0045f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c009fa-7387-423d-9e1d-3ef49cc4f109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5438c8-c31f-4c39-9b2a-6860de383c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec4b64b-e71b-4d42-89a9-8de054e76fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d6aa6a-4294-4ff6-bfd1-9f8ccc52028c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937d0c8-695f-4e70-b0d1-515b7974cbde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
