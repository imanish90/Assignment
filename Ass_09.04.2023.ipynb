{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3bb67-c285-4c2e-b2f6-5df8d37b8085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f3f85-ec80-442a-b732-8bc5366a366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem is a fundamental concept in probability theory and statistics. It describes the probability of an event based on prior knowledge or information. Specifically, it provides a way to calculate the probability of an event A occurring given the occurrence of another event B, by taking into account the prior probability of A and B, and the conditional probability of B given A.\n",
    "\n",
    "Mathematically, Bayes' theorem can be expressed as:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "where:\n",
    "P(A|B) is the probability of A given B,\n",
    "P(B|A) is the probability of B given A,\n",
    "P(A) is the prior probability of A, and\n",
    "P(B) is the prior probability of B.\n",
    "\n",
    "Bayes' theorem is named after Reverend Thomas Bayes, an 18th century British statistician and theologian who first formulated the theorem. It has numerous applications in various fields, including medicine, engineering, finance, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035cee5-ec2d-464f-b3c6-e3642dd5033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dea093-c766-427d-8d03-c43e401f0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "The formula for Bayes' theorem is:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "where:\n",
    "\n",
    "P(A|B) is the probability of event A occurring given that event B has occurred.\n",
    "P(B|A) is the probability of event B occurring given that event A has occurred.\n",
    "P(A) is the prior probability of event A occurring.\n",
    "P(B) is the prior probability of event B occurring.\n",
    "In words, the formula states that the probability of event A occurring given that event B has occurred is equal to the probability of event B occurring given that event A has occurred multiplied by the prior probability of event A, divided by the prior probability of event B.\n",
    "\n",
    "Bayes' theorem is used in a wide range of applications, such as medical diagnosis, spam filtering, and predictive modeling in machine learning. It provides a framework for updating probabilities based on new information, allowing for more accurate predictions and decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7011cf-0548-4169-9abb-8d68680942ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f7873-fb85-4a64-9597-ee94a1cb6c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem is used in many practical applications in a variety of fields. Here are a few examples:\n",
    "\n",
    "Medical Diagnosis: Bayes' theorem is used to calculate the probability of a patient having a disease given a positive test result. The prior probability of the patient having the disease is combined with the probability of a positive test result given the patient has the disease, and the probability of a positive test result given the patient does not have the disease. This calculation can help doctors determine the likelihood of a patient having a disease and make more informed decisions about treatment.\n",
    "\n",
    "Spam Filtering: Bayes' theorem is used in spam filtering algorithms to classify emails as spam or not spam. The algorithm calculates the probability of an email being spam given certain characteristics of the email (such as the presence of certain words or phrases), taking into account prior probabilities based on previously classified emails.\n",
    "\n",
    "Predictive Modeling: Bayes' theorem is used in machine learning for predictive modeling. The theorem can be used to update the probability distribution of a model's parameters given new data, allowing the model to make more accurate predictions.\n",
    "\n",
    "Fault Diagnosis: Bayes' theorem is used in fault diagnosis of systems like aircraft, automobiles, and power plants. It can help identify the root cause of a problem by taking into account prior knowledge of the system's behavior and the symptoms observed during a failure.\n",
    "\n",
    "These are just a few examples of how Bayes' theorem is used in practice. It is a powerful tool for making decisions and predictions based on data and prior knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a2f825-2b32-481f-8d8a-87e1bb06155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9fd0e8-b671-4a7c-bd50-ea5878a7c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem and conditional probability are closely related concepts. Conditional probability is the probability of an event occurring given that another event has occurred. Bayes' theorem provides a way to calculate conditional probabilities by taking into account prior probabilities and new information.\n",
    "\n",
    "In Bayes' theorem, the conditional probability of event A given event B is calculated as follows:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "where P(A|B) is the conditional probability of A given B, P(B|A) is the conditional probability of B given A, P(A) is the prior probability of A, and P(B) is the prior probability of B.\n",
    "\n",
    "The formula for conditional probability is similar, but without taking prior probabilities into account:\n",
    "\n",
    "P(A|B) = P(A and B) / P(B)\n",
    "\n",
    "where P(A and B) is the joint probability of events A and B occurring, and P(B) is the probability of event B occurring.\n",
    "\n",
    "So, Bayes' theorem is a more general form of conditional probability that takes into account prior knowledge and can be used to update probabilities based on new information. Conditional probability is a special case of Bayes' theorem when there is no prior knowledge or additional information available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8749e416-dfd6-4336-bd29-b169b4adce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96aff93-8122-4ab0-aee7-fd8dffa301d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are three types of Naive Bayes classifiers: Gaussian, Multinomial, and Bernoulli. Each type is suited for different types of data and classification tasks.\n",
    "\n",
    "Gaussian Naive Bayes: This type of classifier is suitable for continuous data that can be modeled using a Gaussian (normal) distribution. It is often used for classification tasks in which the features are continuous and normally distributed, such as image recognition and natural language processing.\n",
    "\n",
    "Multinomial Naive Bayes: This type of classifier is suitable for discrete data that can be represented as counts or frequencies. It is often used for text classification tasks, such as spam filtering or sentiment analysis, in which the features are represented as word counts or frequency distributions.\n",
    "\n",
    "Bernoulli Naive Bayes: This type of classifier is similar to Multinomial Naive Bayes, but it is used when the features are binary (0/1) instead of counts. It is often used for binary text classification tasks, such as sentiment analysis or spam filtering.\n",
    "\n",
    "In general, the choice of which type of Naive Bayes classifier to use depends on the nature of the data and the classification task at hand. Here are some factors to consider:\n",
    "\n",
    "Type of data: Gaussian Naive Bayes is suited for continuous data, while Multinomial and Bernoulli Naive Bayes are suited for discrete data.\n",
    "Number of features: Multinomial and Bernoulli Naive Bayes are often used for text classification tasks with a large number of features (e.g., many unique words), while Gaussian Naive Bayes may be less effective in such cases.\n",
    "Sparsity of data: Multinomial and Bernoulli Naive Bayes are better suited for sparse data (i.e., when many features have zero values), while Gaussian Naive Bayes may be less effective in such cases.\n",
    "It is important to note that the choice of Naive Bayes classifier is not always straightforward and may require experimentation and evaluation of different models on the specific problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f899bb95-af75-47a3-a8ec-c354adb01296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094db687-5d89-42a0-9c80-4871ee005878",
   "metadata": {},
   "outputs": [],
   "source": [
    "To use Naive Bayes to classify the new instance with X1=3 and X2=4, we need to calculate the posterior probabilities of each class given these feature values. We can use the following formula to calculate the posterior probabilities:\n",
    "\n",
    "P(class|X1=3,X2=4) = P(X1=3,X2=4|class) * P(class) / P(X1=3,X2=4)\n",
    "\n",
    "where P(class) is the prior probability of the class, P(X1=3,X2=4|class) is the likelihood of observing the feature values given the class, and P(X1=3,X2=4) is the marginal probability of observing the feature values.\n",
    "\n",
    "Since we are assuming equal prior probabilities for each class, P(class) is 0.5 for both classes A and B. To calculate the likelihoods, we can use the Naive Bayes assumption that the features are conditionally independent given the class. This means that we can calculate the likelihood of observing the feature values for each feature separately and then multiply them together.\n",
    "\n",
    "For feature X1=3, the likelihoods for classes A and B are:\n",
    "\n",
    "P(X1=3|A) = 4/13\n",
    "P(X1=3|B) = 1/7\n",
    "\n",
    "For feature X2=4, the likelihoods for classes A and B are:\n",
    "\n",
    "P(X2=4|A) = 3/13\n",
    "P(X2=4|B) = 1/3\n",
    "\n",
    "To calculate the marginal probability P(X1=3,X2=4), we can use the law of total probability:\n",
    "\n",
    "P(X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) + P(X1=3,X2=4|B) * P(B)\n",
    "\n",
    "= (4/13 * 3/13 * 0.5) + (1/7 * 1/3 * 0.5)\n",
    "\n",
    "= 0.0279 + 0.0238\n",
    "\n",
    "= 0.0517\n",
    "\n",
    "Now we can calculate the posterior probabilities for each class:\n",
    "\n",
    "P(A|X1=3,X2=4) = (4/13 * 3/13 * 0.5) / 0.0517 = 0.4459\n",
    "P(B|X1=3,X2=4) = (1/7 * 1/3 * 0.5) / 0.0517 = 0.5541\n",
    "\n",
    "Therefore, Naive Bayes would predict that the new instance with X1=3 and X2=4 belongs to class B, since it has a higher posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49ad709-82ff-4371-a94c-b83f8e5d7484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8f1aa4-b837-4c44-b8d6-0071c32ad907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d4634-9120-4b54-9dec-9c00f838eb41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f6f57-57f8-4b7c-8348-8e10962dcb5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54abeb18-4704-4558-b723-70d518946353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb36fb5-e154-4db6-bb50-14e52ae41362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d23f1f-af1e-4bd1-be3a-090178b54d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823445db-8a73-430e-87c5-fe90fbf90136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d8e20-0a44-4e3d-ad37-e2c5a68a7327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8857e585-6343-4d38-8ab4-57c9eeb780a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6d51ca-2be2-48f0-a225-bd7e8634c2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
