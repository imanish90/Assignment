{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006fe26e-2b08-416d-a6e9-36e0fdddfe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c8ee2-2b16-4eed-b495-90202e2623bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing values in a dataset refer to the absence of a value or information in a particular column or row of a dataset. Missing values can occur due to various reasons such as errors in data collection, data entry, or data processing, or because of missing responses from survey participants.\n",
    "\n",
    "It is essential to handle missing values in a dataset because missing data can cause biases, reduce statistical power, and affect the accuracy and reliability of data analysis. Missing data can lead to biased estimates of means, variances, and other statistical measures, which can impact the results of any data analysis or modeling.\n",
    "\n",
    "Some of the algorithms that are not affected by missing values are:\n",
    "\n",
    "Tree-based algorithms: Decision trees, Random Forest, and Gradient Boosted Trees are some of the popular algorithms that are not affected by missing values as they can handle missing values during the model training process.\n",
    "\n",
    "K-Nearest Neighbors (KNN): KNN is a non-parametric algorithm that can work with missing values, as it imputes the missing values by taking the average of the K-nearest neighbors.\n",
    "\n",
    "Support Vector Machines (SVM): SVM can handle missing values by imputing them with the mean or median value of the non-missing data in the same column.\n",
    "\n",
    "Naive Bayes: Naive Bayes is a probabilistic algorithm that can handle missing data by ignoring the missing values and only considering the available data.\n",
    "\n",
    "Principal Component Analysis (PCA): PCA is a dimensionality reduction technique that can handle missing values by using the available data to estimate the missing values and then using this estimated data to calculate the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e2923d-ec30-405b-b2ab-8677e8cda924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f0f4b4-5fc9-4043-88a8-26ff28a3f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are various techniques used to handle missing data, some of the popular ones are:\n",
    "\n",
    "Deletion: In this technique, the missing values are removed from the dataset. There are three types of deletion techniques:\n",
    "\n",
    "a. Listwise deletion: In this technique, entire rows with missing values are removed.\n",
    "\n",
    "b. Pairwise deletion: In this technique, only the missing values in each pair of variables are removed.\n",
    "\n",
    "c. Dropping variables: In this technique, variables with too many missing values are removed.\n",
    "\n",
    "Here's an example of how to perform pairwise deletion using pandas in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c980f72-b8c7-4c1a-92f3-42e4ce71d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataframe\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "# Perform pairwise deletion\n",
    "df_pairwise = df.dropna()\n",
    "\n",
    "# Print the result\n",
    "print(df_pairwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2194cc99-86d9-42ba-a16c-5bc7fe2532c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean/Median/Mode Imputation: In this technique, the missing values are replaced with the mean, median, or mode of the available data.\n",
    "Here's an example of how to perform mean imputation using scikit-learn in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add363c7-f7b9-4939-887e-0fe6742ef147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataframe\n",
    "data = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\n",
    "\n",
    "# Define the imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit and transform the data\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "# Print the result\n",
    "print(imputed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9f9e3d-baf1-493e-8f84-c5bdd7341400",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regression Imputation: In this technique, the missing values are predicted using a regression model based on the available data.\n",
    "Here's an example of how to perform regression imputation using scikit-learn in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204ea54-feef-4302-91e5-ea7a1476d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataframe\n",
    "data = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\n",
    "\n",
    "# Define the imputer\n",
    "imputer = IterativeImputer()\n",
    "\n",
    "# Fit and transform the data\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "# Print the result\n",
    "print(imputed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d965f2-58ed-4cb7-a6d6-be22ea6b1d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "K-Nearest Neighbor Imputation: In this technique, the missing values are replaced with the values of the K-nearest neighbors based on the available data.\n",
    "Here's an example of how to perform KNN imputation using fancyimpute in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacdd9c4-8f74-43bc-b37e-3250b41d13da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fancyimpute import KNN\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataframe\n",
    "data = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\n",
    "\n",
    "# Define the imputer\n",
    "imputer = KNN()\n",
    "\n",
    "# Fit and transform the data\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "# Print the result\n",
    "print(imputed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e92b67-fabb-430f-92be-a10ed189f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiple Imputation: In this technique, the missing values are imputed multiple times, and the results are combined to create a final imputed dataset.\n",
    "Here's an example of how to perform multiple imputation using the missingpy library in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77769c6-5bda-4c40-b311-a641787dfbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from missingpy import MissForest\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataframe\n",
    "data = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\n",
    "\n",
    "# Define the im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6906a-b67e-4c34-9c0c-ee85dfa2bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3464b4c-18fc-4469-975c-0431479b5822",
   "metadata": {},
   "outputs": [],
   "source": [
    "Imbalanced data refers to a situation in a dataset where the number of observations in each class or category is not equal or proportional. For example, if a dataset contains two classes, and one class has significantly fewer observations than the other, the data is considered imbalanced. Imbalanced data is a common problem in various fields, such as fraud detection, disease diagnosis, and credit scoring.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to biased models and inaccurate predictions. The model may become too focused on the majority class and neglect the minority class, resulting in poor performance and incorrect predictions for the minority class. In some cases, the model may even predict only the majority class, resulting in a useless model.\n",
    "\n",
    "Moreover, in such scenarios, accuracy is not a good performance metric to evaluate the model's performance, as even a model that predicts only the majority class will have high accuracy. Instead, other metrics, such as precision, recall, F1-score, and AUC-ROC, are better suited for evaluating the performance of models on imbalanced data.\n",
    "\n",
    "Therefore, it is crucial to handle imbalanced data to ensure that the model can accurately predict outcomes for all classes. Some of the techniques used to handle imbalanced data are undersampling, oversampling, and a combination of both techniques. Additionally, advanced algorithms like Random Forest, Gradient Boosted Trees, and XGBoost can also handle imbalanced data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31720c6-4e2a-4d4f-adbd-ab84d4b965b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a98d73-bbb7-434e-85ce-99233058a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Up-sampling and down-sampling are two techniques used to handle imbalanced data.\n",
    "\n",
    "Down-sampling, also known as under-sampling, involves randomly removing observations from the majority class to balance the class distribution. For example, if a dataset contains 1000 observations of class A and 100 observations of class B, down-sampling will randomly remove some of the observations of class A, so that both classes have the same number of observations.\n",
    "\n",
    "Up-sampling, also known as over-sampling, involves increasing the number of observations in the minority class to balance the class distribution. For example, if a dataset contains 1000 observations of class A and 100 observations of class B, up-sampling will create synthetic observations of class B so that both classes have the same number of observations.\n",
    "\n",
    "Here's an example of when up-sampling and down-sampling might be required:\n",
    "\n",
    "Suppose we have a dataset of credit card transactions, where the positive class represents fraudulent transactions, and the negative class represents legitimate transactions. If the dataset contains 90% legitimate transactions and only 10% fraudulent transactions, the data is imbalanced. In such a scenario, the model may become too focused on the majority class (legitimate transactions) and neglect the minority class (fraudulent transactions), leading to poor performance.\n",
    "\n",
    "In this case, we can use up-sampling to increase the number of fraudulent transactions in the dataset by creating synthetic fraudulent transactions. Alternatively, we can use down-sampling to reduce the number of legitimate transactions in the dataset by randomly removing some of the legitimate transactions. Both techniques can help to balance the class distribution and improve the performance of the model.\n",
    "\n",
    "However, it's important to note that both up-sampling and down-sampling have their drawbacks. Up-sampling may result in overfitting, as the model may learn from synthetic observations that do not reflect the true distribution of the minority class. Down-sampling may result in a loss of information, as some of the observations in the majority class are removed. Therefore, it's essential to evaluate the performance of the model using appropriate metrics and choose the best technique based on the specific scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75e08bf-98b4-4071-852c-60b79e5e1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adab106-e3fe-44a5-bce2-b93ecb8bb9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data augmentation is a technique used to increase the size of a dataset by creating new data from existing data. The goal of data augmentation is to improve the performance of a machine learning model by providing it with more training data. Data augmentation is commonly used in computer vision, natural language processing, and other fields where large amounts of data are required for training models.\n",
    "\n",
    "SMOTE, which stands for Synthetic Minority Over-sampling Technique, is a type of data augmentation technique used to handle imbalanced data. SMOTE creates synthetic samples of the minority class by selecting two or more similar observations from the minority class and creating a new observation between them. The new observation is a linear combination of the selected observations, with the features of the observation randomly perturbed. By doing so, SMOTE creates new observations that reflect the distribution of the minority class, which can help to balance the class distribution and improve the performance of a model.\n",
    "\n",
    "Here's an example of how SMOTE works:\n",
    "\n",
    "Suppose we have a dataset of credit card transactions, where the positive class represents fraudulent transactions, and the negative class represents legitimate transactions. If the dataset contains 90% legitimate transactions and only 10% fraudulent transactions, the data is imbalanced. To balance the class distribution, we can use SMOTE to create synthetic fraudulent transactions.\n",
    "\n",
    "To use SMOTE, we first select a minority class observation and find its k nearest neighbors in the feature space. We then randomly select one of the k nearest neighbors and create a new observation that is a linear combination of the two observations. Finally, we repeat this process until the desired number of synthetic observations has been created.\n",
    "\n",
    "For example, suppose we select a fraudulent transaction with a credit limit of $10,000 and a time of day of 2:00 AM. One of its k nearest neighbors is a fraudulent transaction with a credit limit of $12,000 and a time of day of 1:30 AM. We can create a new synthetic observation by taking a weighted average of the two observations:\n",
    "\n",
    "Credit limit: (1 - w) * 10,000 + w * 12,000 = 11,000\n",
    "Time of day: (1 - w) * 2:00 AM + w * 1:30 AM = 1:45 AM\n",
    "where w is a random number between 0 and 1. By creating new observations in this way, SMOTE can help to balance the class distribution and improve the performance of a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d090e-6446-43d4-a1c1-86427e081a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98bcca1-d055-4f86-8f38-07828eb60cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Outliers are data points that are significantly different from other data points in a dataset. They can occur due to measurement errors, experimental errors, or other anomalies. Outliers can have a significant impact on the statistical analysis of a dataset, as they can distort the estimates of central tendency, variance, and correlations. Therefore, it is essential to handle outliers to ensure that the statistical analysis of a dataset is accurate and reliable.\n",
    "\n",
    "There are several reasons why it is important to handle outliers:\n",
    "\n",
    "Impact on descriptive statistics: Outliers can affect the mean, median, and mode of a dataset, which are measures of central tendency. If the outliers are not handled, these measures may not accurately reflect the true distribution of the data.\n",
    "\n",
    "Impact on inferential statistics: Outliers can also affect the estimates of variance, standard deviation, and other measures of dispersion. These measures are used in inferential statistics to test hypotheses and make predictions. If the outliers are not handled, the estimates of variance and standard deviation may be biased, which can lead to incorrect conclusions.\n",
    "\n",
    "Impact on machine learning models: Outliers can also affect the performance of machine learning models. Machine learning models are often based on statistical techniques, and outliers can affect the estimates of parameters and the performance of the model. Therefore, it is important to handle outliers to ensure that the machine learning model is accurate and reliable.\n",
    "\n",
    "There are several techniques used to handle outliers, such as:\n",
    "\n",
    "Removing outliers: One approach is to remove outliers from the dataset entirely. This approach is simple but can lead to a loss of information if the outliers are important for the analysis.\n",
    "\n",
    "Winsorizing: This approach involves replacing the outliers with the highest or lowest non-outlying value in the dataset. This approach is less extreme than removing outliers entirely and can preserve some of the information contained in the outliers.\n",
    "\n",
    "Robust statistics: Another approach is to use robust statistical methods that are less sensitive to outliers, such as the median and the interquartile range (IQR).\n",
    "\n",
    "Overall, handling outliers is an essential step in statistical analysis and machine learning to ensure that the analysis is accurate and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6302c5b-8d90-45f8-96fc-d966881d69e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b237255-4cea-4e30-b4f6-c9181923a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several techniques that can be used to handle missing data in a dataset, some of which are:\n",
    "\n",
    "Deletion: One approach to handling missing data is to delete the rows or columns that contain the missing values. This approach is straightforward but can lead to a loss of information if the deleted rows or columns are important for the analysis.\n",
    "\n",
    "Imputation: Imputation involves filling in the missing values with estimated values based on the available data. There are several imputation techniques that can be used, including mean imputation, median imputation, mode imputation, regression imputation, and K-nearest neighbors (KNN) imputation.\n",
    "\n",
    "Prediction: If the missing data is the target variable that needs to be predicted, machine learning models can be used to predict the missing values based on the available data.\n",
    "\n",
    "Multiple Imputation: This technique is useful for imputing missing data in complex datasets with many variables. It involves creating several imputed datasets using different imputation methods and then analyzing each dataset separately to obtain a final analysis result.\n",
    "\n",
    "The choice of technique for handling missing data depends on the nature of the dataset and the analysis being conducted. For instance, deletion may be appropriate if the amount of missing data is small and randomly distributed. On the other hand, imputation may be appropriate if the missing data is non-random or is a significant proportion of the dataset.\n",
    "\n",
    "In summary, handling missing data is essential to ensure that the analysis is accurate and reliable. It is important to carefully consider the techniques available and choose the appropriate technique for the specific dataset and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4355afe-776c-4901-8994-690dae729c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11b27e-42a7-4ebe-a93a-a375429bcb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several strategies that can be used to determine if missing data is missing at random (MAR) or if there is a pattern to the missing data, such as:\n",
    "\n",
    "Missing Data Analysis: This involves examining the missing data to determine if there are any patterns or trends. For instance, if the missing data is related to a specific variable, it may indicate that the missing data is not missing at random.\n",
    "\n",
    "Statistical Tests: Statistical tests can be used to determine if the missing data is missing at random. For example, the Little's MCAR test can be used to test whether the missing data is missing completely at random (MCAR), while the Missing Indicator method can be used to test whether the missing data is missing at random (MAR).\n",
    "\n",
    "Imputation Techniques: Imputation techniques can also provide insights into whether the missing data is MAR or not. For example, if mean imputation results in similar estimates as regression imputation, it may indicate that the missing data is MAR.\n",
    "\n",
    "Domain Knowledge: Domain knowledge can also be used to determine if the missing data is MAR. For instance, if the missing data is related to a variable that is known to be associated with a specific group, it may indicate that the missing data is not MAR.\n",
    "\n",
    "In summary, determining if missing data is MAR or not is essential to ensure that the analysis is accurate and reliable. It is important to use multiple strategies and consider domain knowledge to make an informed decision about the nature of the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e94dbfa-57f4-4612-8dd4-121a74e642d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb859b45-c6ce-4455-a956-8f94d19b8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Imbalanced datasets pose a challenge for machine learning models as they tend to bias towards the majority class. Here are some strategies to evaluate the performance of a machine learning model on an imbalanced dataset:\n",
    "\n",
    "Confusion Matrix: The confusion matrix provides a breakdown of the model's predictions into true positives, false positives, true negatives, and false negatives. It can be used to calculate various performance metrics such as precision, recall, F1-score, and accuracy.\n",
    "\n",
    "ROC Curve: The Receiver Operating Characteristic (ROC) curve plots the true positive rate against the false positive rate at various classification thresholds. It is a useful tool to evaluate the model's performance across different thresholds and select an optimal threshold.\n",
    "\n",
    "Precision-Recall Curve: The precision-recall curve plots precision against recall at various classification thresholds. It is a useful tool to evaluate the model's performance in cases where the positive class is rare.\n",
    "\n",
    "Class Weights: One way to address class imbalance is to use class weights during model training. Class weights increase the weight of the minority class during training and can improve the model's performance on the minority class.\n",
    "\n",
    "Resampling Techniques: Resampling techniques such as over-sampling the minority class (e.g., SMOTE) or under-sampling the majority class (e.g., random under-sampling) can be used to balance the class distribution in the training data.\n",
    "\n",
    "In summary, imbalanced datasets require special attention during the evaluation of machine learning models. It is important to use appropriate evaluation metrics, explore different classification thresholds, and consider resampling techniques to balance the class distribution during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb21e8ec-b42d-4005-86cf-6994b7c3676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f438c1dc-a175-4c01-88c6-7712782494f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "When dealing with an imbalanced dataset, particularly one where the majority class is over-represented, there are several methods that can be employed to balance the dataset and down-sample the majority class. These include:\n",
    "\n",
    "Random under-sampling: This involves randomly selecting a subset of the majority class samples to match the size of the minority class. The drawback of this method is that it can result in the loss of useful information.\n",
    "Here is an example of how to use the imbalanced-learn library in Python to implement random under-sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a235b-b02f-40de-912c-cb7d8a12c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a390c4a-d184-43f1-a70c-c047a4909199",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tomek links: This method involves removing samples from the majority class that are near samples from the minority class. This can help to create more separation between the classes.\n",
    "Here is an example of how to use the imbalanced-learn library in Python to implement Tomek links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd2d56-66bf-4f79-afc9-7da060c2c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "tl = TomekLinks()\n",
    "X_resampled, y_resampled = tl.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f437a-8987-402d-9745-e8dc4255125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster-based under-sampling: This method involves clustering the majority class samples and removing samples from clusters that have a large number of samples. This can help to preserve the overall distribution of the majority class while reducing the number of samples.\n",
    "Here is an example of how to use the imbalanced-learn library in Python to implement cluster-based under-sampling:\n",
    "\n",
    "python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71c59d4-fef0-4d79-9cfb-83f0ac75a1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "cc = ClusterCentroids()\n",
    "X_resampled, y_resampled = cc.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2395dd0-55d7-46f2-bf8a-7b6330ea0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "In summary, random under-sampling, Tomek links, and cluster-based under-sampling are all effective methods for balancing an imbalanced dataset and down-sampling the majority class. Each method has its strengths and weaknesses, so it's important to experiment with different methods and evaluate their performance. The imbalanced-learn library in Python provides a convenient way to implement these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec4e700-9f3c-42af-bb9c-bb2fe4904a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c7d0de-a19a-4c75-8783-0562f304570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "When dealing with an imbalanced dataset where the minority class has a low percentage of occurrences, there are several methods that can be employed to balance the dataset and up-sample the minority class. These include:\n",
    "\n",
    "Random over-sampling: This involves randomly duplicating samples from the minority class to match the size of the majority class. The drawback of this method is that it can result in overfitting.\n",
    "Here is an example of how to use the imbalanced-learn library in Python to implement random over-sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1811fb23-235b-44a6-862d-e3260c469f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860a943-2af5-493f-b285-391b904d2798",
   "metadata": {},
   "outputs": [],
   "source": [
    "Synthetic minority over-sampling technique (SMOTE): This method involves creating synthetic samples by interpolating between existing minority class samples. This can help to create more separation between the classes.\n",
    "Here is an example of how to use the imbalanced-learn library in Python to implement SMOTE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4cfb0-a47b-4395-956b-241a68b2580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb915f9c-3d1d-4ad3-a891-59e49e5d56c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adaptive synthetic (ADASYN): This method is an extension of SMOTE that generates synthetic samples adaptively by adding more synthetic samples to the minority class examples that are harder to learn.\n",
    "Here is an example of how to use the imbalanced-learn library in Python to implement ADASYN:\n",
    "\n",
    "python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e220d92a-0b24-40f2-9e04-23b3306d17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "adasyn = ADASYN()\n",
    "X_resampled, y_resampled = adasyn.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab75c9-9c77-41b0-a09a-e622944d74ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "In summary, random over-sampling, SMOTE, and ADASYN are all effective methods for balancing an imbalanced dataset and up-sampling the minority class. Each method has its strengths and weaknesses, so it's important to experiment with different methods and evaluate their performance. The imbalanced-learn library in Python provides a convenient way to implement these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc910a8-e5ea-4e83-a208-f6c3f6d81485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
