{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa89be-e9bc-4ff9-8f38-e29c1adefe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a340ef-dfb4-4b9e-93ea-4be41fa0bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the context of artificial neural networks, an activation function is a mathematical function that determines the output of a neuron or node. It introduces non-linearity into the network, allowing it to learn and approximate complex relationships between inputs and outputs.\n",
    "\n",
    "Each neuron in an artificial neural network receives inputs, applies the activation function to the weighted sum of those inputs, and produces an output. This output is then passed to the next layer of neurons as inputs. The activation function decides whether the neuron should be \"activated\" or not, i.e., whether it should fire and transmit its signal further.\n",
    "\n",
    "The activation function serves two primary purposes in a neural network:\n",
    "\n",
    "Introducing non-linearity: Without activation functions, the neural network would simply be a series of linear transformations, resulting in a linear model. Non-linear activation functions allow the network to learn and represent complex, non-linear relationships between inputs and outputs, enabling it to solve more intricate problems.\n",
    "\n",
    "Decision-making: The activation function determines the output of a neuron based on the weighted sum of its inputs. It sets a threshold or boundary for the neuron's activation, determining whether it should be \"on\" or \"off.\" This decision-making capability allows the neural network to learn and make predictions.\n",
    "\n",
    "Commonly used activation functions include:\n",
    "\n",
    "Sigmoid function: It maps the input to a value between 0 and 1, providing a smooth activation. It is commonly used in the output layer for binary classification problems.\n",
    "\n",
    "Hyperbolic tangent (tanh) function: Similar to the sigmoid function, but it maps the input to a value between -1 and 1, offering a centered activation.\n",
    "\n",
    "Rectified Linear Unit (ReLU) function: It sets negative inputs to zero and leaves positive inputs unchanged. ReLU is widely used in hidden layers as it is computationally efficient and mitigates the vanishing gradient problem.\n",
    "\n",
    "Leaky ReLU: Similar to ReLU, but it introduces a small slope for negative inputs, preventing neurons from dying completely.\n",
    "\n",
    "Softmax function: Primarily used in multi-class classification problems, it computes the probability distribution over multiple classes, ensuring the sum of the probabilities is 1.\n",
    "\n",
    "The choice of activation function depends on the specific problem at hand, network architecture, and other factors. Different activation functions have different properties and can impact the training dynamics, convergence, and overall performance of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b63b7b1-68a1-4775-8bc2-c07405585702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07133e3-8b6f-4a53-a0d7-10e74717dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several commonly used activation functions in neural networks. Here are some of them:\n",
    "\n",
    "Sigmoid Function: The sigmoid function, also known as the logistic function, maps the input to a value between 0 and 1. It has an S-shaped curve and is given by the formula:\n",
    "\n",
    "f(x) = 1 / (1 + e^(-x))\n",
    "\n",
    "The sigmoid function is widely used in the past for binary classification tasks, but it has fallen out of favor for hidden layers due to the vanishing gradient problem.\n",
    "\n",
    "Hyperbolic Tangent (tanh) Function: The hyperbolic tangent function is similar to the sigmoid function, but it maps the input to a value between -1 and 1. It is given by the formula:\n",
    "\n",
    "f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "\n",
    "Like the sigmoid function, tanh also suffers from the vanishing gradient problem, but it is centered around zero and provides stronger gradients for inputs in the linear region.\n",
    "\n",
    "Rectified Linear Unit (ReLU): The ReLU function is defined as f(x) = max(0, x), which means it sets negative inputs to zero and leaves positive inputs unchanged. ReLU has become one of the most popular activation functions in deep learning due to its simplicity and ability to mitigate the vanishing gradient problem. It provides fast computation and encourages sparse activation.\n",
    "\n",
    "Leaky ReLU: The leaky ReLU is a variation of the ReLU function that introduces a small slope for negative inputs instead of setting them to zero. It is defined as f(x) = max(ax, x), where a is a small positive constant. The purpose of the leaky ReLU is to prevent neurons from dying completely and address the \"dying ReLU\" problem.\n",
    "\n",
    "Parametric ReLU (PReLU): PReLU is an extension of leaky ReLU where the small slope is learned during training instead of being a fixed parameter. It allows the network to adaptively determine the slope for negative inputs.\n",
    "\n",
    "Softmax Function: The softmax function is primarily used in multi-class classification problems. It takes a vector of real numbers as input and transforms them into a probability distribution over multiple classes, ensuring that the sum of the probabilities is 1. The softmax function is defined as:\n",
    "\n",
    "f(x_i) = e^(x_i) / (sum(e^(x_j)) for j=1 to n)\n",
    "\n",
    "These are some of the commonly used activation functions in neural networks. The choice of activation function depends on the problem, network architecture, and other factors, and experimentation with different activation functions is often necessary to achieve the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90adf221-841d-449c-9b10-b8bb08984103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79880f41-094d-422a-adf0-d1cdbd32f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "Activation functions play a crucial role in the training process and performance of a neural network. Here are some ways in which activation functions can impact neural network training:\n",
    "\n",
    "Non-linearity: Activation functions introduce non-linearity into the network, enabling it to learn and represent complex, non-linear relationships in the data. Linear activation functions would result in a network that is equivalent to a linear model, severely limiting its expressive power. Non-linear activation functions allow the network to model intricate patterns and make accurate predictions.\n",
    "\n",
    "Gradient Flow: The choice of activation function affects the flow of gradients during backpropagation, which is crucial for updating the network's weights. Ideally, activation functions should have gradients that neither vanish nor explode as they propagate through multiple layers. Activation functions with vanishing gradients can impede learning, especially in deep networks, as the gradients become extremely small and fail to update the earlier layers effectively. On the other hand, activation functions with exploding gradients can lead to unstable training. Activation functions like ReLU and its variants (e.g., leaky ReLU) have gradients that can avoid the vanishing gradient problem to some extent.\n",
    "\n",
    "Computational Efficiency: The computational efficiency of activation functions can impact the overall training time of the neural network. Some activation functions, such as ReLU, are computationally efficient as they involve simple mathematical operations. This efficiency becomes crucial when dealing with large-scale datasets and complex architectures.\n",
    "\n",
    "Avoiding Saturation: Saturation refers to the situation where the activations of neurons are pushed towards extreme values (e.g., 0 or 1) due to the activation function. Activation functions like sigmoid and tanh are prone to saturation when the inputs are large. Saturation can hinder the training process as it limits the gradients and slows down learning. ReLU and its variants are less likely to suffer from saturation for positive inputs.\n",
    "\n",
    "Output Range and Interpretability: The output range of an activation function can impact the interpretation of the network's output. For example, sigmoid and softmax functions produce outputs that can be interpreted as probabilities, while ReLU produces unbounded positive values. The choice of activation function should align with the desired output range and interpretation of the problem at hand.\n",
    "\n",
    "Generalization and Overfitting: Different activation functions can affect the generalization ability of the network. Some activation functions, such as ReLU, can introduce sparsity by setting negative values to zero. This sparsity can prevent overfitting by reducing the network's capacity to memorize training data, promoting more robust and generalizable representations.\n",
    "\n",
    "Choosing the appropriate activation function is a crucial aspect of designing neural networks. It often requires experimentation and consideration of the specific problem, network architecture, and other factors. Different activation functions have their strengths and weaknesses, and selecting the most suitable one can significantly impact the performance and convergence of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d753ea1-9631-49c1-87e5-1ed3a296697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5fe1c-bf20-4fc2-8111-98897f4a5534",
   "metadata": {},
   "outputs": [],
   "source": [
    "The sigmoid activation function, also known as the logistic function, is a non-linear function that maps the input to a value between 0 and 1. It has an S-shaped curve and is given by the formula:\n",
    "\n",
    "f(x) = 1 / (1 + e^(-x))\n",
    "\n",
    "Here's how the sigmoid activation function works:\n",
    "\n",
    "Range: The sigmoid function's output always falls between 0 and 1, making it suitable for tasks where the output needs to be interpreted as a probability or a binary classification decision.\n",
    "\n",
    "Non-linearity: The sigmoid function introduces non-linearity into the network. It allows the neural network to learn and represent complex, non-linear relationships between inputs and outputs. Without non-linear activation functions like sigmoid, a neural network would be reduced to a series of linear transformations, limiting its ability to model intricate patterns in the data.\n",
    "\n",
    "Advantages of the sigmoid activation function:\n",
    "\n",
    "Smoothness: The sigmoid function is a smooth and differentiable function. Its smoothness ensures continuous changes in output as the input varies, facilitating gradient-based optimization algorithms like backpropagation to update the network's weights efficiently.\n",
    "\n",
    "Probability Interpretation: The output of the sigmoid function can be interpreted as a probability. In binary classification problems, the sigmoid function can represent the probability of an input belonging to a specific class. This interpretation is particularly useful in tasks such as logistic regression, where the goal is to estimate class probabilities.\n",
    "\n",
    "Disadvantages of the sigmoid activation function:\n",
    "\n",
    "Vanishing Gradient: The sigmoid function suffers from the vanishing gradient problem. As the input moves away from zero in either positive or negative direction, the derivative of the sigmoid becomes very small, approaching zero. This makes it challenging for the gradients to propagate back through many layers during training. In deep neural networks, the vanishing gradient problem can hinder learning in the early layers, slowing down convergence or preventing it altogether.\n",
    "\n",
    "Output Saturation: The sigmoid function tends to saturate when the input values are very large or very small. As the input becomes extremely positive or negative, the output of the sigmoid approaches 1 or 0, respectively. This saturation can lead to the loss of gradient information and hinder learning. It can also make the network less sensitive to changes in the input.\n",
    "\n",
    "Biased Output: The sigmoid function maps inputs to a range between 0 and 1. Consequently, the outputs tend to be biased towards the extreme ends of the range. This bias can cause problems in the learning process, especially when the data distribution is imbalanced.\n",
    "\n",
    "Due to these disadvantages, the sigmoid activation function has been largely replaced by other activation functions like ReLU (Rectified Linear Unit) and its variants in many neural network architectures. ReLU overcomes the vanishing gradient problem, is computationally efficient, and has become the default choice for hidden layers in deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebece100-c34f-4b3d-a6ae-008da7d804dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47032f0a-2f8f-4480-9256-69153fe904bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Rectified Linear Unit (ReLU) activation function is a popular non-linear function used in neural networks. It is defined as follows:\n",
    "\n",
    "f(x) = max(0, x)\n",
    "\n",
    "In other words, the ReLU function sets negative values of the input to zero and leaves positive values unchanged.\n",
    "\n",
    "Here's how the ReLU activation function differs from the sigmoid function:\n",
    "\n",
    "Range: The range of the ReLU function is from 0 to positive infinity. In contrast, the sigmoid function maps the input to a range between 0 and 1. ReLU does not squash the input values into a specific range, allowing the activations to be unbounded and positive.\n",
    "\n",
    "Non-linearity: Both the sigmoid and ReLU functions introduce non-linearity into the neural network. However, the nature of their non-linearity differs. The sigmoid function has a smooth, sigmoid-shaped curve, while the ReLU function introduces a sharp, linear non-linearity. This linearity makes ReLU computationally efficient and allows for faster convergence during training.\n",
    "\n",
    "Sparsity: One key characteristic of the ReLU function is that it introduces sparsity by setting negative values to zero. This sparsity property makes ReLU well-suited for scenarios where sparse representations are desirable. By promoting sparsity, ReLU can help prevent overfitting by reducing the network's capacity to memorize training data and encouraging more robust and generalizable representations.\n",
    "\n",
    "Vanishing Gradient: The ReLU activation function helps alleviate the vanishing gradient problem, which can occur in deep neural networks. The vanishing gradient problem refers to the issue where the gradients in the earlier layers of a network become extremely small, making it difficult for the network to learn effectively. Since ReLU does not saturate for positive inputs, it allows gradients to flow more easily, avoiding the vanishing gradient problem to a large extent.\n",
    "\n",
    "Differentiability: One limitation of the ReLU activation function is that it is not differentiable at the point where x = 0. However, in practice, this is not usually an issue as the function is differentiable almost everywhere and can be handled in optimization algorithms like backpropagation.\n",
    "\n",
    "Bias towards positive values: ReLU does not have an explicit upper limit on its output, which means it does not squash positive values. As a result, ReLU tends to produce positive activations, which can make the network more sensitive to positive inputs.\n",
    "\n",
    "Overall, the ReLU activation function offers computational efficiency, addresses the vanishing gradient problem, and introduces sparsity. These advantages have made ReLU a popular choice as an activation function, especially in deep neural networks, where it has shown to improve training and performance compared to functions like the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9527b28-3b19-4f04-b45d-aceddb8b9062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6264ada-b719-4e34-befb-2752aebb022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using the Rectified Linear Unit (ReLU) activation function over the sigmoid function offers several benefits. Here are some advantages of ReLU:\n",
    "\n",
    "Avoiding the Vanishing Gradient Problem: The vanishing gradient problem occurs when the gradients in the earlier layers of a deep neural network become extremely small during backpropagation, hindering the learning process. ReLU helps mitigate this problem as it does not saturate for positive inputs. The derivative of ReLU is either 1 or 0, resulting in more effective gradient flow. This allows gradients to propagate more easily and helps in training deep neural networks with many layers.\n",
    "\n",
    "Computational Efficiency: ReLU is computationally efficient compared to the sigmoid function. The ReLU activation simply sets negative values to zero, which involves a simple thresholding operation. This simplicity leads to faster computation, making ReLU particularly advantageous when training large-scale neural networks with millions of parameters.\n",
    "\n",
    "Sparsity and Overfitting Prevention: ReLU introduces sparsity in the network by setting negative values to zero. This sparsity property makes the activations more sparse, reducing the number of active neurons in the network. By promoting sparsity, ReLU can prevent overfitting by reducing the network's capacity to memorize training data and encouraging more generalizable representations.\n",
    "\n",
    "Addressing the Gradient Saturation Problem: The sigmoid function can suffer from gradient saturation, where the gradients become extremely small in the saturated regions of the function. This saturation can make learning slower and hinder convergence. ReLU avoids this issue as it does not saturate in the positive region, allowing for stronger gradients and faster learning.\n",
    "\n",
    "Linear Separability: ReLU can enhance the linear separability of classes in the feature space. By keeping positive values intact and setting negative values to zero, ReLU effectively creates a linear decision boundary between the positive and negative regions. This property can aid in the learning of linearly separable patterns.\n",
    "\n",
    "Sparse Activation Representation: ReLU's sparsity property can lead to more efficient representations in the network. By having a subset of active neurons, ReLU reduces redundant computations and memory requirements, making the network more memory-efficient and easier to train.\n",
    "\n",
    "It is important to note that the choice of activation function depends on the specific problem, network architecture, and other factors. While ReLU offers significant advantages, the sigmoid function may still be suitable for certain tasks such as binary classification where the output needs to be interpreted as probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e5f6a-9ad2-45b9-9aac-de8364fde96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d918575-f26c-4e02-b4b3-51532a2c4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Leaky Rectified Linear Unit (Leaky ReLU) is a variant of the Rectified Linear Unit (ReLU) activation function. It was introduced to address one of the limitations of the standard ReLU function, known as the \"dying ReLU\" problem, which occurs when a large portion of the ReLU neurons becomes inactive and outputs zero for any input.\n",
    "\n",
    "The Leaky ReLU function is defined as follows:\n",
    "\n",
    "f(x) = max(ax, x)\n",
    "\n",
    "where 'a' is a small positive constant, typically set to a small value like 0.01. In the Leaky ReLU, negative values are not set to zero, but instead, they are scaled by a small slope 'a'. This small slope introduces a small gradient for negative inputs, allowing some information to flow even for negative values.\n",
    "\n",
    "By introducing this small non-zero gradient for negative inputs, Leaky ReLU prevents neurons from completely \"dying\" as they can still carry some information during backpropagation. This helps in mitigating the vanishing gradient problem associated with the standard ReLU function, particularly in deep neural networks with many layers.\n",
    "\n",
    "The small slope 'a' in the Leaky ReLU is typically kept very small to ensure that the non-linearity introduced by the function is primarily governed by the positive inputs. This prevents the negative inputs from dominating and affecting the network's learning ability. The small slope 'a' is usually set as a hyperparameter and can be adjusted during the training process using techniques such as backpropagation.\n",
    "\n",
    "The Leaky ReLU activation function has gained popularity due to its ability to address the vanishing gradient problem while maintaining the computational efficiency and sparsity properties of the standard ReLU function. It allows the network to learn from negative inputs, prevents neurons from becoming completely inactive, and provides a more robust gradient flow during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853e7227-137d-48dc-b137-edb9c6c736e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bbc9fc-de9c-4d32-af83-ccb70b45a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "The softmax activation function is commonly used in neural networks, particularly in multi-class classification tasks. It is designed to transform the outputs of a neural network into a probability distribution over multiple classes. The purpose of the softmax function is to provide a normalized representation of the network's outputs that can be interpreted as class probabilities.\n",
    "\n",
    "The softmax function takes a vector of real-valued inputs and produces a vector of values that sum up to 1, with each value representing the probability of the corresponding class. It is defined as follows for a vector of inputs x:\n",
    "\n",
    "softmax(x_i) = exp(x_i) / sum(exp(x_j)) for all j\n",
    "\n",
    "Here's the purpose and common usage of the softmax activation function:\n",
    "\n",
    "Probability Interpretation: The softmax function maps the output values of a neural network to a probability distribution. It ensures that the predicted probabilities for each class are non-negative and sum up to 1. This probability interpretation is particularly useful in multi-class classification tasks, where the goal is to assign an input to one of several possible classes.\n",
    "\n",
    "Multi-Class Classification: Softmax is commonly used as the final activation function in the output layer of a neural network for multi-class classification problems. It allows the network to provide class probabilities, enabling it to make predictions by selecting the class with the highest probability.\n",
    "\n",
    "Training with Cross-Entropy Loss: Softmax is often used in conjunction with the cross-entropy loss function during training. The softmax outputs serve as inputs to the cross-entropy loss, which measures the dissimilarity between the predicted probabilities and the true class labels. The combination of softmax and cross-entropy loss is well-suited for optimizing the network's parameters to minimize classification error in multi-class problems.\n",
    "\n",
    "Ensuring Mutual Exclusivity: Softmax assumes mutual exclusivity among the classes, meaning that an input can belong to only one class. The probabilities assigned by softmax are mutually exclusive and do not allow for overlap between classes. This property makes softmax suitable for problems where an input can be assigned to one and only one class, such as image classification or sentiment analysis.\n",
    "\n",
    "It's important to note that softmax is sensitive to the relative magnitudes of the input values. Extremely large or small input values can result in almost certain predictions for certain classes, leading to potential issues with numerical stability and overconfidence in predictions. Techniques like temperature scaling can be applied to control the output distribution and adjust the balance between exploration and exploitation in probabilistic predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f33bb2e-09e6-4b22-9e79-a09dd8c0394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ab5e8-b0d9-4ee8-af7c-d03986199597",
   "metadata": {},
   "outputs": [],
   "source": [
    "The hyperbolic tangent (tanh) activation function is a non-linear function that maps the input to a value between -1 and 1. It is derived from the sigmoid function but has a different range and shape. The tanh function is defined as:\n",
    "\n",
    "tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "\n",
    "Here's how the tanh activation function compares to the sigmoid function:\n",
    "\n",
    "Range: The sigmoid function maps the input to a range between 0 and 1, while the tanh function maps the input to a range between -1 and 1. The tanh function is symmetric around the origin, which means it can produce negative outputs as well. This property can be useful in situations where negative values are meaningful or when the data exhibits symmetry.\n",
    "\n",
    "Non-linearity: Both the sigmoid and tanh functions introduce non-linearity into neural networks, allowing them to model complex, non-linear relationships between inputs and outputs. However, the shape of the tanh function differs from the sigmoid function. The tanh function has a steeper gradient around the origin, leading to more pronounced non-linearity.\n",
    "\n",
    "Zero-Centered: One advantage of the tanh function over the sigmoid function is that it is zero-centered. The output of the tanh function is centered around zero, meaning that the average output of the function is close to zero when the inputs are close to zero. This zero-centered property can help with the convergence of the network during training, especially when dealing with data that has zero-mean.\n",
    "\n",
    "Gradient Magnitude: The gradient of the tanh function is larger than the gradient of the sigmoid function in the region around the origin. This larger gradient can result in more pronounced updates to the network's weights during backpropagation and can lead to faster convergence, particularly when the input values are not saturated.\n",
    "\n",
    "Saturation and Vanishing Gradient: Similar to the sigmoid function, the tanh function can also suffer from saturation when the input values are very large or very small. As the input moves away from zero, the tanh function saturates, approaching -1 or 1. This saturation can cause the gradients to become very small, leading to the vanishing gradient problem, especially in deep neural networks.\n",
    "\n",
    "Similarities: The sigmoid and tanh functions are both smooth, differentiable functions that are widely used in various neural network architectures. They have similar properties such as being non-linear and facilitating the use of gradient-based optimization algorithms like backpropagation.\n",
    "\n",
    "While the sigmoid function has been largely replaced by the ReLU family of activation functions in many neural network architectures, the tanh function is still used in certain contexts, such as recurrent neural networks (RNNs) or when zero-centered outputs are desired. The choice between the sigmoid and tanh functions depends on the specific requirements of the task, the characteristics of the data, and the desired properties of the network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
