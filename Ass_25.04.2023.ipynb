{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e51f92-f97b-46c6-97df-5821557d1de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee6f7a-155e-46ff-b1de-fd5b788a5c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eigenvalues and eigenvectors are important concepts in linear algebra and have a wide range of applications in physics, engineering, and computer science.\n",
    "\n",
    "An eigenvector is a non-zero vector that, when multiplied by a matrix, results in a scalar multiple of itself. In other words, if we have a matrix A and a vector v, and if Av = λv for some scalar λ, then v is called an eigenvector of A and λ is called its corresponding eigenvalue.\n",
    "\n",
    "The eigen-decomposition approach is a technique used to decompose a matrix A into its eigenvalues and eigenvectors. It is also known as the spectral decomposition or eigendecomposition. In this approach, we diagonalize a matrix by finding its eigenvalues and eigenvectors.\n",
    "\n",
    "For example, let's consider a 2x2 matrix A:\n",
    "A = [[2, 1],\n",
    "     [1, 2]]\n",
    "To find the eigenvalues of A, we solve the characteristic equation:\n",
    "    det(A - λI) = 0\n",
    "    \n",
    "    where I is the identity matrix and det is the determinant. So, for A, the characteristic equation is:\n",
    "        det([[2, 1], [1, 2]] - λ[[1, 0], [0, 1]]) = 0\n",
    "        \n",
    "        which simplifies to:\n",
    "            (2-λ)² - 1 = 0\n",
    "Solving this equation gives us two eigenvalues: λ1 = 1 and λ2 = 3.\n",
    "\n",
    "Next, we find the eigenvectors corresponding to each eigenvalue. For λ1 = 1, we solve the equation:\n",
    "    (A - λ1I)v1 = 0\n",
    "    \n",
    "which gives us:\n",
    "    [[1, 1],\n",
    " [1, 1]]v1 = 0\n",
    "    \n",
    "Solving this equation, we get v1 = [1, -1] as the eigenvector corresponding to λ1 = 1.\n",
    "\n",
    "Similarly, for λ2 = 3, we solve the equation:\n",
    "    (A - λ2I)v2 = 0\n",
    "which gives us:\n",
    "    [[-1, 1],\n",
    " [1, -1]]v2 = 0\n",
    "    \n",
    "Solving this equation, we get v2 = [1, 1] as the eigenvector corresponding to λ2 = 3.\n",
    "\n",
    "Finally, we can write A as a product of its eigenvectors and eigenvalues:\n",
    "A = PDP^-1\n",
    "\n",
    "where P is a matrix whose columns are the eigenvectors of A, and D is a diagonal matrix whose diagonal entries are the eigenvalues of A. In this case, we have:\n",
    "\n",
    "    P = [[1, 1],\n",
    "     [-1, 1]]\n",
    "\n",
    "D = [[1, 0],\n",
    "     [0, 3]]\n",
    "\n",
    "So, the eigen-decomposition of A is:\n",
    "    A = [[2, 1],\n",
    "     [1, 2]] = [[1, 1],\n",
    "               [-1, 1]] [[1, 0],\n",
    "                          [0, 3]] [[1, -1],\n",
    "                                     [1, 1]]^-1\n",
    "\n",
    "Eigenvalues and eigenvectors have many applications in data analysis, machine learning, and image processing, among others. The eigenvalues tell us about the scaling of the matrix along the eigenvectors, and the eigenvectors tell us about the direction of the scaling. By decomposing a matrix into its eigenvectors and eigenvalues, we can gain insight into its structure and properties, and use this information to solve problems in various fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f428c86-f4f8-4fe8-bc53-6f672867607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1613b55-c918-488c-9796-a1db739d631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eigen decomposition, also known as eigendecomposition or spectral decomposition, is a fundamental concept in linear algebra. It involves decomposing a square matrix into a set of eigenvalues and eigenvectors.\n",
    "\n",
    "The eigenvalues of a matrix are scalars that represent the stretching or shrinking of the corresponding eigenvectors when the matrix is applied to them. An eigenvector of a matrix is a non-zero vector that, when multiplied by the matrix, results in a scalar multiple of itself.\n",
    "\n",
    "The eigen decomposition of a matrix A can be written as:\n",
    "\n",
    "A = VDV^-1\n",
    "\n",
    "where V is a matrix whose columns are the eigenvectors of A, D is a diagonal matrix whose diagonal entries are the eigenvalues of A, and V^-1 is the inverse of V.\n",
    "\n",
    "Eigen decomposition is significant in linear algebra because it provides a way to diagonalize a matrix. A diagonal matrix has all of its off-diagonal entries equal to zero, which makes it easier to perform computations such as matrix multiplication, inversion, and computing matrix powers.\n",
    "\n",
    "Furthermore, eigen decomposition is also used in a variety of applications, such as in image compression, principal component analysis, and solving differential equations. In image compression, for example, eigen decomposition is used to identify the principal components of an image, which can then be used to represent the image using fewer bits of information. In principal component analysis, eigen decomposition is used to identify the most important features of a dataset, which can then be used to reduce the dimensionality of the dataset.\n",
    "\n",
    "Overall, eigen decomposition is a powerful tool in linear algebra that provides insights into the structure and properties of a matrix, and is widely used in a variety of applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d89282-bdc4-407e-9ee4-d5ab184a63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d94bb3-92d9-44b4-a444-8dfe08d34d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "A square matrix is diagonalizable if and only if it has n linearly independent eigenvectors, where n is the dimension of the matrix.\n",
    "\n",
    "Proof:\n",
    "\n",
    "Let A be a square matrix of size n. Suppose A is diagonalizable, then there exists a diagonal matrix D and an invertible matrix P such that A = PDP^-1, where D contains the eigenvalues of A on its diagonal and P contains the eigenvectors of A as its columns.\n",
    "\n",
    "We know that for each eigenvalue λ of A, there exists a nonzero vector x such that Ax = λx. This implies that x is an eigenvector of A corresponding to the eigenvalue λ. Hence, the eigenvectors of A are linearly independent, since they correspond to distinct eigenvalues.\n",
    "\n",
    "Conversely, suppose A has n linearly independent eigenvectors, say {v1, v2, ..., vn}, with corresponding eigenvalues {λ1, λ2, ..., λn}. We can construct the matrix P whose columns are the eigenvectors of A. Then, we have AP = PD, where D is the diagonal matrix with the eigenvalues on its diagonal. This implies that A = PDP^-1, so A is diagonalizable.\n",
    "\n",
    "Therefore, a square matrix A is diagonalizable if and only if it has n linearly independent eigenvectors, where n is the dimension of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1295419-cc49-4585-a935-8e1a53204749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408718a1-9b6d-406c-a135-3d72ef3eb925",
   "metadata": {},
   "outputs": [],
   "source": [
    "The spectral theorem is a fundamental result in linear algebra that provides a connection between the eigenvalues and eigenvectors of a Hermitian or symmetric matrix, and the diagonalization of the matrix. In the context of the Eigen-Decomposition approach, the spectral theorem guarantees that any Hermitian or symmetric matrix can be diagonalized by a unitary matrix, which is a matrix whose columns are orthonormal.\n",
    "\n",
    "The spectral theorem is related to the diagonalizability of a matrix because it states that a matrix is diagonalizable if and only if it is Hermitian or symmetric. This means that if a matrix is Hermitian or symmetric, then it can be diagonalized using the Eigen-Decomposition approach.\n",
    "\n",
    "For example, consider the following symmetric matrix A:\n",
    "\n",
    "A = [ 3 1 1 ; 1 2 0 ; 1 0 2 ]\n",
    "\n",
    "The Eigen-Decomposition approach can be used to find the eigenvectors and eigenvalues of A:\n",
    "\n",
    "The characteristic polynomial of A is det(A - λI) = (3-λ)((2-λ)^2 - 1) = (3-λ)(λ-1)(λ-3).\n",
    "\n",
    "The eigenvalues of A are λ1 = 3, λ2 = 1, and λ3 = 2.\n",
    "\n",
    "To find the eigenvectors, we substitute each eigenvalue into the equation (A - λI)x = 0 and solve for x.\n",
    "\n",
    "For λ1 = 3, we have (A - 3I)x = 0, which gives the solution x1 = [ 1 ; 1 ; 1 ].\n",
    "\n",
    "For λ2 = 1, we have (A - I)x = 0, which gives the solution x2 = [ 1 ; -1 ; 0 ].\n",
    "\n",
    "For λ3 = 2, we have (A - 2I)x = 0, which gives the solution x3 = [ 0 ; 1 ; -1 ].\n",
    "\n",
    "The eigenvectors x1, x2, and x3 are orthogonal, so we can construct the unitary matrix P whose columns are the eigenvectors:\n",
    "\n",
    "P = [ 1/√3 1/√2 0 ; 1/√3 -1/√2 1/√2 ; 1/√3 0 -1/√2 ]\n",
    "\n",
    "The diagonal matrix D containing the eigenvalues can be constructed as:\n",
    "\n",
    "D = [ 3 0 0 ; 0 1 0 ; 0 0 2 ]\n",
    "\n",
    "Therefore, we have A = PDP^-1, and A is diagonalizable using the Eigen-Decomposition approach.\n",
    "\n",
    "In summary, the spectral theorem is an important result that relates the eigenvalues and eigenvectors of a Hermitian or symmetric matrix to its diagonalization using the Eigen-Decomposition approach. If a matrix is Hermitian or symmetric, then it can be diagonalized using the Eigen-Decomposition approach, and the resulting diagonal matrix will contain the eigenvalues of the matrix on its diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab37294e-e519-4dd5-b2cc-f5ebf0527e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c13fd-3fe6-444b-8d31-277c8a8d8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "To find the eigenvalues of a matrix, we solve the characteristic equation det(A - λI) = 0, where A is the matrix, λ is a scalar, and I is the identity matrix of the same size as A. The eigenvalues are the solutions to this equation.\n",
    "\n",
    "The eigenvalues of a matrix represent the scalar values that, when multiplied by the corresponding eigenvectors, give back the same vector, up to a scalar multiple. More formally, if v is an eigenvector of A with eigenvalue λ, then Av = λv. The eigenvalue λ tells us how the corresponding eigenvector v is scaled when multiplied by the matrix A.\n",
    "\n",
    "Eigenvalues are important in many applications of linear algebra, such as systems of linear differential equations, matrix diagonalization, and principal component analysis. They also provide information about the matrix properties, such as its determinant and trace.\n",
    "\n",
    "For example, consider the matrix A = [ 1 2 ; 2 1 ]. To find the eigenvalues of A, we solve det(A - λI) = 0, which gives us the characteristic equation λ^2 - 2λ - 3 = 0. Solving this equation, we get the eigenvalues λ1 = 3 and λ2 = -1.\n",
    "\n",
    "These eigenvalues represent the scaling factors of the corresponding eigenvectors. To find the eigenvectors, we substitute each eigenvalue back into the equation (A - λI)x = 0 and solve for x. For λ1 = 3, we have (A - 3I)x = 0, which gives us the solution x1 = [ 1 ; 1 ]. For λ2 = -1, we have (A + I)x = 0, which gives us the solution x2 = [ -1 ; 1 ].\n",
    "\n",
    "Therefore, the eigenvalues of A are λ1 = 3 and λ2 = -1, and the corresponding eigenvectors are x1 = [ 1 ; 1 ] and x2 = [ -1 ; 1 ], respectively. These eigenvalues and eigenvectors can be used to diagonalize A, and provide information about the properties of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a62fb-19e6-4431-bdb2-1a1e3207700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9d2957-8f36-4eeb-b982-b6b92a449e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eigenvectors are non-zero vectors that, when multiplied by a matrix, are scaled by a scalar value known as the eigenvalue. More formally, let A be an n x n square matrix and λ be a scalar. A non-zero vector v is an eigenvector of A corresponding to the eigenvalue λ if Av = λv. In other words, multiplying the matrix A by the eigenvector v results in a scalar multiple of the same vector v.\n",
    "\n",
    "Eigenvectors and eigenvalues are related because every eigenvalue of a matrix corresponds to a set of eigenvectors. Specifically, the set of eigenvectors corresponding to a particular eigenvalue λ is the null space of the matrix (A - λI), where I is the n x n identity matrix. This means that the eigenvectors are the non-zero solutions to the homogeneous system of linear equations (A - λI)x = 0.\n",
    "\n",
    "Eigenvalues and eigenvectors are important in many areas of mathematics and science. In linear algebra, they are used for diagonalization of matrices, computing matrix powers, and solving differential equations. In data analysis, eigenvectors and eigenvalues are used for principal component analysis, a technique for reducing the dimensionality of data.\n",
    "\n",
    "For example, consider the matrix A = [ 1 2 ; 2 1 ]. The eigenvalues of A are λ1 = 3 and λ2 = -1, as we saw in the previous question. The corresponding eigenvectors are x1 = [ 1 ; 1 ] and x2 = [ -1 ; 1 ], respectively. We can verify that these vectors are indeed eigenvectors of A by computing Av for each vector and comparing the result to λv. We have:\n",
    "\n",
    "A x1 = [ 1 2 ; 2 1 ] [ 1 ; 1 ] = [ 3 ; 3 ] = 3 [ 1 ; 1 ] = λ1 x1\n",
    "A x2 = [ 1 2 ; 2 1 ] [ -1 ; 1 ] = [ -1 ; 1 ] = -1 [ -1 ; 1 ] = λ2 x2\n",
    "\n",
    "Therefore, x1 and x2 are eigenvectors of A corresponding to the eigenvalues λ1 and λ2, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa906a3b-7624-4d6f-9e93-7bccca56c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9b88f5-477c-47c3-8ebd-163ff32b3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, there is a geometric interpretation of eigenvectors and eigenvalues that is useful in understanding their significance.\n",
    "\n",
    "Geometrically, an eigenvector of a matrix represents a direction in space that is unchanged by a linear transformation. Specifically, if A is a matrix and v is an eigenvector of A, then Av is a vector that points in the same direction as v, but may be scaled by a factor λ, which is the corresponding eigenvalue. This means that applying the matrix transformation to the eigenvector simply stretches or shrinks the vector, without changing its direction.\n",
    "\n",
    "The eigenvalue λ represents the scale factor by which the eigenvector is stretched or shrunk under the matrix transformation. If λ > 1, then the eigenvector is stretched in the direction of v. If 0 < λ < 1, then the eigenvector is shrunk in the direction of v. If λ = 1, then the eigenvector is unchanged by the matrix transformation. If λ < 0, then the eigenvector is flipped in direction.\n",
    "\n",
    "In general, a matrix can have multiple eigenvectors with different eigenvalues. Each eigenvector represents a direction in space that is unchanged by the matrix transformation, and the corresponding eigenvalue represents the scale factor by which the eigenvector is stretched or shrunk in that direction.\n",
    "\n",
    "For example, consider the matrix A = [ 2 1 ; 1 2 ]. The eigenvectors of A are x1 = [ 1 ; 1 ] and x2 = [ -1 ; 1 ], corresponding to the eigenvalues λ1 = 3 and λ2 = 1, respectively. Geometrically, x1 represents a direction in space that is unchanged by A, and is stretched by a factor of 3. x2 represents a direction that is unchanged by A, and is not stretched or shrunk.\n",
    "\n",
    "The eigenvectors and eigenvalues of a matrix provide valuable information about the matrix transformation and its effect on vectors in space. They can be used to decompose the matrix into simpler forms, to compute matrix powers and exponentials, and to analyze data in various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c3ea72-0180-45ea-b972-92398cfb8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ee7da-4100-433a-9b57-6c4a49f05c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eigen decomposition has numerous real-world applications in a variety of fields, including engineering, physics, computer science, and data analysis. Here are some examples:\n",
    "\n",
    "Image processing: In image processing, eigen decomposition can be used for image compression and feature extraction. Eigenfaces is an example of a technique that uses eigen decomposition to extract features from facial images, which can then be used for face recognition.\n",
    "\n",
    "Quantum mechanics: Eigen decomposition is used to solve Schrödinger's equation in quantum mechanics. The eigenvalues and eigenvectors of the Hamiltonian operator represent the possible energy levels and states of a quantum system.\n",
    "\n",
    "Mechanical engineering: Eigen decomposition is used in mechanical engineering to analyze the vibrational modes of structures. The eigenvectors of the mass and stiffness matrices of a structure represent the mode shapes, and the eigenvalues represent the natural frequencies of vibration.\n",
    "\n",
    "Data analysis: Eigen decomposition is used in data analysis for principal component analysis (PCA). PCA is a technique for reducing the dimensionality of data by finding the eigenvectors and eigenvalues of the covariance matrix of the data.\n",
    "\n",
    "Signal processing: Eigen decomposition is used in signal processing for spectral analysis. The eigenvectors and eigenvalues of a signal covariance matrix can be used to extract information about the frequency components of the signal.\n",
    "\n",
    "Social networks: Eigen decomposition is used in social network analysis for finding the most important nodes in a network. The eigenvector centrality of a node is a measure of its importance based on its connections to other important nodes in the network.\n",
    "\n",
    "These are just a few examples of the many applications of eigen decomposition. It is a powerful tool that has broad applications in science and engineering, and is essential for solving many mathematical and computational problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966dd6a5-e43b-4ec6-9f45-7ac5d9bbed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf18097-9277-4699-b7d4-825baf347750",
   "metadata": {},
   "outputs": [],
   "source": [
    "No, a square matrix can have at most n distinct eigenvalues, where n is the dimension of the matrix. Additionally, each eigenvalue can have at most n linearly independent eigenvectors associated with it.\n",
    "\n",
    "It is possible for a matrix to have fewer than n linearly independent eigenvectors, but it cannot have more than n linearly independent eigenvectors associated with any eigenvalue. This is because eigenvectors corresponding to the same eigenvalue are linearly dependent.\n",
    "\n",
    "In some cases, a matrix may have repeated eigenvalues, meaning that the same eigenvalue appears more than once. In this case, there may be fewer than n linearly independent eigenvectors associated with each eigenvalue. The number of linearly independent eigenvectors associated with each eigenvalue is called its geometric multiplicity. The sum of the geometric multiplicities of all eigenvalues is always equal to n.\n",
    "\n",
    "For example, consider the matrix A = [ 2 0 ; 0 2 ]. This matrix has two eigenvalues, λ1 = 2 and λ2 = 2, each with geometric multiplicity 1. The eigenvectors associated with λ1 are x1 = [ 1 ; 0 ] and x2 = [ 0 ; 1 ], both of which are linearly independent. Since each eigenvalue has geometric multiplicity 1, the matrix has a total of two linearly independent eigenvectors.\n",
    "\n",
    "In summary, a square matrix can have at most n distinct eigenvalues and each eigenvalue can have at most n linearly independent eigenvectors associated with it. If an eigenvalue appears more than once, its geometric multiplicity may be less than n, but the sum of the geometric multiplicities of all eigenvalues is always equal to n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b15e6b-9ac7-4a01-8f88-84621987dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa1971a-cf23-475b-809e-5daa784c1a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eigen-Decomposition is a powerful technique that is widely used in data analysis and machine learning for a variety of applications. Here are three specific applications or techniques that rely on Eigen-Decomposition:\n",
    "\n",
    "Principal Component Analysis (PCA): PCA is a technique for reducing the dimensionality of data by finding the most important features or variables that explain the most variance in the data. It involves finding the eigenvectors and eigenvalues of the covariance matrix of the data. The eigenvectors represent the principal components of the data, and the eigenvalues represent the amount of variance explained by each principal component. PCA is widely used in data preprocessing, visualization, and feature extraction in machine learning.\n",
    "\n",
    "Singular Value Decomposition (SVD): SVD is a generalization of Eigen-Decomposition that can be applied to any matrix, not just symmetric matrices. SVD decomposes a matrix into three matrices: U, Σ, and V, where U and V are orthogonal matrices and Σ is a diagonal matrix. SVD is used in a variety of machine learning applications, such as collaborative filtering, matrix factorization, and text analysis.\n",
    "\n",
    "Eigenfaces: Eigenfaces is a facial recognition technique that uses Eigen-Decomposition to extract the most important features from facial images. The technique involves finding the eigenvectors and eigenvalues of the covariance matrix of a set of facial images. The eigenvectors, or eigenfaces, represent the principal components of the facial images. New facial images can be represented as linear combinations of these eigenfaces, and the weights of the linear combinations can be used as features for classification.\n",
    "\n",
    "In summary, Eigen-Decomposition is a versatile technique that is widely used in data analysis and machine learning. PCA, SVD, and Eigenfaces are just a few examples of the many applications and techniques that rely on Eigen-Decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b5607-91c9-4ebb-aa49-696761285d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40213e9-28e7-4c66-a4fe-17a68c4181c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c69ec-f760-4c66-ba65-e60e3b88b7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a646c5-d121-4eca-a956-3cf5622ef763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a5a169-9afb-40fd-9221-2f1e9dd61077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff7e5f7-b74a-473d-87a1-aefe9a053f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac10dc-2287-4555-9e7d-507335a6ebf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5651c69-c0ee-4b11-822a-a2b8bffb3a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b27316d-984e-4a96-9c05-ab5bb2dfce4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
